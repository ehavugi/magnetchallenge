{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvxR-HDER9p9"
      },
      "source": [
        "\n",
        "Connect Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxkFj17oR84b",
        "outputId": "31a2a714-345f-44e6-a133-679e9e8fac56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQzQz2mkMqL6"
      },
      "source": [
        "# Step 0: Import Packages\n",
        "In this demo, the neural network is synthesized using the PyTorch framework. Please install PyTorch according to the [official guidance](https://pytorch.org/get-started/locally/) , then import PyTorch and other dependent modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IdDUEup6490",
        "outputId": "9594cbc0-9c1b-4dc4-c1eb-d0cb660e7957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "import optuna\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import csv\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9uAIku9M0Th"
      },
      "source": [
        "# Step 1: Define Network Structure\n",
        "In this part, we define the structure of the feedforward neural network. Refer to the [PyTorch document](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q5LA1kae860Q"
      },
      "outputs": [],
      "source": [
        "# Initialize a neural network model with a specified number of layers and units in each layer.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_layers, n_units_list):\n",
        "        super(Net, self).__init__()\n",
        "        layers = []\n",
        "        in_size = 1026\n",
        "        for i in range(n_layers):\n",
        "            out_size = n_units_list[i]\n",
        "            layers.append(nn.Linear(in_size, out_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            in_size = out_size\n",
        "        layers.append(nn.Linear(in_size, 1))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxpYPTvPM6eY"
      },
      "source": [
        "# Step 2: Load the Dataset\n",
        "In this part, we load and pre-process the dataset for the network training and testing. In this demo, a small dataset containing triangular waveforms measured with N49ferrite material under different frequency, flux density, and duty ratio is used, which can be downloaded from the [MagNet GitHub](https://github.com/PrincetonUniversity/Magnet) repository under \"tutorial\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y-6sTpOoAUWZ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "\n",
        "# Define file paths for the input features and output target\n",
        "B_file_path = '/content/drive/MyDrive/Material_A/B_Field.csv'\n",
        "Freq_file_path = '/content/drive/MyDrive/Material_A/Frequency.csv'\n",
        "Temp_file_path = '/content/drive/MyDrive/Material_A/Temperature.csv'\n",
        "Power_file_path = '/content/drive/MyDrive/Material_A/Volumetric_Loss.csv'\n",
        "\n",
        "def get_dataset():\n",
        "    # Read the input features and output target from CSV files\n",
        "    B = read_csv(B_file_path)\n",
        "    Freq = read_csv(Freq_file_path)\n",
        "    Temp = read_csv(Temp_file_path)\n",
        "    Power = read_csv(Power_file_path)\n",
        "\n",
        "    # Apply logarithmic transformation to Frequency and Power for better training\n",
        "    Freq = np.log10(Freq)\n",
        "    Temp = np.array(Temp)\n",
        "    Power = np.log10(Power)\n",
        "\n",
        "    # Reshape data into appropriate tensors\n",
        "    Freq = torch.from_numpy(Freq).float().view(-1, 1)\n",
        "    B = torch.from_numpy(B).float().view((-1, 1024, 1))\n",
        "    Temp = torch.from_numpy(Temp).view(-1, 1)\n",
        "    Power = Power.reshape((-1, 1))\n",
        "\n",
        "    # Normalize input features\n",
        "    B = (B - torch.mean(B)) / torch.std(B).numpy()\n",
        "    Freq = (Freq - torch.mean(Freq)) / torch.std(Freq).numpy()\n",
        "    Temp = (Temp - torch.mean(Temp)) / torch.std(Temp).numpy()\n",
        "\n",
        "    # Remove singleton dimension from B tensor\n",
        "    B = np.squeeze(B, axis=2)\n",
        "\n",
        "    # Display the shapes of the input features and output target tensors\n",
        "    print(np.shape(Freq))\n",
        "    print(np.shape(B))\n",
        "    print(np.shape(Temp))\n",
        "    print(np.shape(Power))\n",
        "\n",
        "    # Concatenate normalized input features into a single tensor\n",
        "    temp = np.concatenate((Freq, B, Temp), axis=1)\n",
        "\n",
        "    # Convert input and output tensors into PyTorch TensorDataset\n",
        "    in_tensors = torch.from_numpy(temp).view(-1, 1026)\n",
        "    out_tensors = torch.from_numpy(Power).view(-1, 1)\n",
        "\n",
        "    return torch.utils.data.TensorDataset(in_tensors, out_tensors)\n",
        "\n",
        "def read_csv(file_path):\n",
        "    # Read data from a CSV file and convert it into a NumPy array\n",
        "    data = []\n",
        "    with open(file_path, 'r', newline='') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        for row in csv_reader:\n",
        "            values = [float(value) for value in row]\n",
        "            data.append(values)\n",
        "    return np.array(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HRhRbOaM_ry"
      },
      "source": [
        "# Step 3: Training and Testing the Model\n",
        "In this part, we program the training and testing procedure of the network model. The loaded dataset is randomly split into training set, validation set, and test set. The output of the training is the state dictionary file (.sd) containing all the trained parameter values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ndXYTq9b9R",
        "outputId": "71c16401-689c-4c4f-c8e0-b432ad273af1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-12-30 17:46:31,770] A new study created in memory with name: no-name-5aa238e0-341f-446d-b6ec-bf717106332c\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2432, 1])\n",
            "torch.Size([2432, 1024])\n",
            "torch.Size([2432, 1])\n",
            "(2432, 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-12-30 17:47:37,572] Trial 0 finished with value: 0.00626459154465925 and parameters: {'n_layers': 4, 'n_units_layer_0': 87, 'n_units_layer_1': 98, 'n_units_layer_2': 47, 'n_units_layer_3': 113}. Best is trial 0 with value: 0.00626459154465925.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best hyperparameters: {'n_layers': 4, 'n_units_layer_0': 87, 'n_units_layer_1': 98, 'n_units_layer_2': 47, 'n_units_layer_3': 113}\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Reproducibility\n",
        "    random.seed(1)\n",
        "    np.random.seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    NUM_EPOCH = 1000\n",
        "    BATCH_SIZE = 128\n",
        "    DECAY_EPOCH = 100\n",
        "    DECAY_RATIO = 0.5\n",
        "    LR_INI = 0.02\n",
        "    best_loss = math.inf\n",
        "    # Select GPU as the default device\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = get_dataset()\n",
        "\n",
        "    # Split the dataset\n",
        "    train_size = int(0.6 * len(dataset))\n",
        "    valid_size = int(0.2 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - valid_size\n",
        "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "    # Setup Optuna study\n",
        "    def objective(trial):\n",
        "        # Sample hyperparameters\n",
        "        n_layers = trial.suggest_int('n_layers', 4, 5)\n",
        "        n_units_list = [trial.suggest_int(f'n_units_layer_{i}', 16, 128, log=False) for i in range(n_layers)]\n",
        "\n",
        "        # Update the network architecture\n",
        "        net = Net(n_layers, n_units_list).double().to(device)\n",
        "\n",
        "        # Setup optimizer\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(net.parameters(), lr=LR_INI)\n",
        "\n",
        "        # Train the network\n",
        "        for epoch_i in range(NUM_EPOCH):\n",
        "            # Train for one epoch\n",
        "            epoch_train_loss = 0\n",
        "            net.train()\n",
        "            optimizer.param_groups[0]['lr'] = LR_INI * (DECAY_RATIO ** (0 + epoch_i // DECAY_EPOCH))\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = net(inputs.to(device))\n",
        "                loss = criterion(outputs, labels.to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_train_loss += loss.item()\n",
        "\n",
        "            # Compute Validation Loss\n",
        "            with torch.no_grad():\n",
        "                epoch_valid_loss = 0\n",
        "                for inputs, labels in valid_loader:\n",
        "                    outputs = net(inputs.to(device))\n",
        "                    loss = criterion(outputs, labels.to(device))\n",
        "                    epoch_valid_loss += loss.item()\n",
        "        return epoch_valid_loss\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=200)\n",
        "\n",
        "    # Get the best hyperparameters\n",
        "    best_params = study.best_params\n",
        "    print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "    # Update the network architecture with the best hyperparameters\n",
        "    best_net = Net(best_params['n_layers'], [best_params[f'n_units_layer_{i}'] for i in range(best_params['n_layers'])]).double().to(device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
