{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connect Google drive"
      ],
      "metadata": {
        "id": "GvxR-HDER9p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxkFj17oR84b",
        "outputId": "65e511dc-4ed5-4976-9157-1ff57f34e973"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Import Packages\n",
        "In this demo, the neural network is synthesized using the PyTorch framework. Please install PyTorch according to the [official guidance](https://pytorch.org/get-started/locally/) , then import PyTorch and other dependent modules."
      ],
      "metadata": {
        "id": "GQzQz2mkMqL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6IdDUEup6490",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08e82cb-066e-474b-c984-000733cd0b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.1 colorlog-6.8.0 optuna-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "!pip install optuna\n",
        "import optuna\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import csv\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Define Network Structure\n",
        "In this part, we define the structure of the feedforward neural network. Refer to the [PyTorch document](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html) for more details."
      ],
      "metadata": {
        "id": "r9uAIku9M0Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model structures and functions\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # Define a fully connected layers model with three inputs (frequency, flux density, duty ratio) and one output (power loss).\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(1026, 65),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(65, 55),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(55, 116),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(116, 40),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(40, 123),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(123, 1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "q5LA1kae860Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the Dataset\n",
        "In this part, we load and pre-process the dataset for the network training and testing. In this demo, a small dataset containing triangular waveforms measured with N49ferrite material under different frequency, flux density, and duty ratio is used, which can be downloaded from the [MagNet GitHub](https://github.com/PrincetonUniversity/Magnet) repository under \"tutorial\"."
      ],
      "metadata": {
        "id": "HxpYPTvPM6eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define file paths for the input features and output target\n",
        "B_file_path = '/content/drive/MyDrive/Material_A/B_Field.csv'\n",
        "Freq_file_path = '/content/drive/MyDrive/Material_A/Frequency.csv'\n",
        "Temp_file_path = '/content/drive/MyDrive/Material_A/Temperature.csv'\n",
        "Power_file_path = '/content/drive/MyDrive/Material_A/Volumetric_Loss.csv'\n",
        "\n",
        "def get_dataset():\n",
        "    # Read the input features and output target from CSV files\n",
        "    B = read_csv(B_file_path)\n",
        "    Freq = read_csv(Freq_file_path)\n",
        "    Temp = read_csv(Temp_file_path)\n",
        "    Power = read_csv(Power_file_path)\n",
        "\n",
        "    # Apply logarithmic transformation to Frequency and Power for better training\n",
        "    Freq = np.log10(Freq)\n",
        "    Temp = np.array(Temp)\n",
        "    Power = np.log10(Power)\n",
        "\n",
        "    # Reshape data into appropriate tensors\n",
        "    Freq = torch.from_numpy(Freq).float().view(-1, 1)\n",
        "    B = torch.from_numpy(B).float().view((-1, 1024, 1))\n",
        "    Temp = torch.from_numpy(Temp).view(-1, 1)\n",
        "    Power = Power.reshape((-1, 1))\n",
        "\n",
        "    # Normalize input features\n",
        "    B = (B - torch.mean(B)) / torch.std(B).numpy()\n",
        "    Freq = (Freq - torch.mean(Freq)) / torch.std(Freq).numpy()\n",
        "    Temp = (Temp - torch.mean(Temp)) / torch.std(Temp).numpy()\n",
        "\n",
        "    # Remove singleton dimension from B tensor\n",
        "    B = np.squeeze(B, axis=2)\n",
        "\n",
        "    # Display the shapes of the input features and output target tensors\n",
        "    print(np.shape(Freq))\n",
        "    print(np.shape(B))\n",
        "    print(np.shape(Temp))\n",
        "    print(np.shape(Power))\n",
        "\n",
        "    # Concatenate normalized input features into a single tensor\n",
        "    temp = np.concatenate((Freq, B, Temp), axis=1)\n",
        "\n",
        "    # Convert input and output tensors into PyTorch TensorDataset\n",
        "    in_tensors = torch.from_numpy(temp).view(-1, 1026)\n",
        "    out_tensors = torch.from_numpy(Power).view(-1, 1)\n",
        "\n",
        "    return torch.utils.data.TensorDataset(in_tensors, out_tensors)\n",
        "\n",
        "def read_csv(file_path):\n",
        "    # Read data from a CSV file and convert it into a NumPy array\n",
        "    data = []\n",
        "    with open(file_path, 'r', newline='') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        for row in csv_reader:\n",
        "            values = [float(value) for value in row]\n",
        "            data.append(values)\n",
        "    return np.array(data)"
      ],
      "metadata": {
        "id": "Y-6sTpOoAUWZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Training and Testing the Model\n",
        "In this part, we program the training and testing procedure of the network model. The loaded dataset is randomly split into training set, validation set, and test set. The output of the training is the state dictionary file (.sd) containing all the trained parameter values."
      ],
      "metadata": {
        "id": "1HRhRbOaM_ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(1)\n",
        "    np.random.seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    NUM_EPOCH = 2000\n",
        "    BATCH_SIZE = 128\n",
        "    LR_INI = 0.005\n",
        "    best_loss = math.inf\n",
        "\n",
        "    # Select GPU as the default device\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = get_dataset()\n",
        "\n",
        "    def objective(trial):\n",
        "        # Hyperparameters to be optimized by Optuna\n",
        "        DECAY_EPOCH = trial.suggest_int('DECAY_EPOCH', 100, 500)\n",
        "        DECAY_RATIO = trial.suggest_float('DECAY_RATIO', 0.1, 0.9)\n",
        "\n",
        "        # Split the dataset into training, validation, and test sets\n",
        "        train_size = int(0.6 * len(dataset))\n",
        "        valid_size = int(0.2 * len(dataset))\n",
        "        test_size = len(dataset) - train_size - valid_size\n",
        "        train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
        "        kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "        # Set up the neural network\n",
        "        net = Net().double().to(device)\n",
        "\n",
        "        # Set up the optimizer and criterion\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(net.parameters(), lr=LR_INI)\n",
        "\n",
        "        for epoch_i in range(NUM_EPOCH):\n",
        "            # Train for one epoch\n",
        "            epoch_train_loss = 0\n",
        "            net.train()\n",
        "            optimizer.param_groups[0]['lr'] = LR_INI * (DECAY_RATIO ** (0 + epoch_i // DECAY_EPOCH))\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = net(inputs.to(device))\n",
        "                loss = criterion(outputs, labels.to(device))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_train_loss += loss.item()\n",
        "\n",
        "            # Compute Validation Loss\n",
        "            with torch.no_grad():\n",
        "                epoch_valid_loss = 0\n",
        "                for inputs, labels in valid_loader:\n",
        "                    outputs = net(inputs.to(device))\n",
        "                    loss = criterion(outputs, labels.to(device))\n",
        "\n",
        "                    epoch_valid_loss += loss.item()\n",
        "\n",
        "        return epoch_valid_loss\n",
        "\n",
        "    # Use Optuna for hyperparameter optimization\n",
        "    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.RandomSampler(seed=1))\n",
        "    study.optimize(objective, n_trials=200)\n",
        "\n",
        "    # Get the best hyperparameters\n",
        "    best_params = study.best_params\n",
        "    print(\"Best Batch Size:\", best_params)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ndXYTq9b9R",
        "outputId": "ccf0c01e-2fa1-45ca-e8ea-ac7c1028cd05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-12-30 17:48:42,844] A new study created in memory with name: no-name-197010e0-831e-4028-97a6-169b28587202\n",
            "[I 2023-12-30 17:51:02,662] Trial 0 finished with value: 0.005424472376502349 and parameters: {'DECAY_EPOCH': 267, 'DECAY_RATIO': 0.6762595947537264}. Best is trial 0 with value: 0.005424472376502349.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Batch Size: {'DECAY_EPOCH': 267, 'DECAY_RATIO': 0.6762595947537264}\n"
          ]
        }
      ]
    }
  ]
}