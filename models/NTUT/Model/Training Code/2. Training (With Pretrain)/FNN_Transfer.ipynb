{"cells":[{"cell_type":"markdown","metadata":{"id":"GvxR-HDER9p9"},"source":["掛接雲端硬碟"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19861,"status":"ok","timestamp":1703996843574,"user":{"displayName":"Yun Shan Hsieh","userId":"04540611790251984553"},"user_tz":-480},"id":"GxkFj17oR84b","outputId":"895ff37e-6c67-4789-a1a3-db3aff56817e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IdDUEup6490"},"outputs":[],"source":["# Import necessary packages\n","\n","import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import random\n","import numpy as np\n","import json\n","import math\n","import csv\n","from pathlib import Path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5LA1kae860Q"},"outputs":[],"source":["# Define model structures and functions\n","\n","class Net(nn.Module):\n","    def __init__(self, load_pretrained: bool = False, pretrained_model_path :str  = \"None\"):\n","        super(Net, self).__init__()\n","        # Define a fully connected layers model with three inputs (frequency, flux density, duty ratio) and one output (power loss).\n","        self.layers = nn.Sequential(\n","            nn.Linear(1026, 65),\n","            nn.ReLU(),\n","            nn.Linear(65, 55),\n","            nn.ReLU(),\n","            nn.Linear(55, 116),\n","            nn.ReLU(),\n","            nn.Linear(116, 40),\n","            nn.ReLU(),\n","            nn.Linear(40, 123),\n","            nn.ReLU(),\n","            nn.Linear(123, 1),\n","        )\n","        if load_pretrained and pretrained_model_path is not None:\n","          self.load_pretrained_model(pretrained_model_path)\n","\n","    def forward(self, x):\n","        return self.layers(x)\n","\n","    def load_pretrained_model(self, path):\n","      pretrained_dict = torch.load(path)\n","      model_dict = self.state_dict()\n","      model_dict.update(pretrained_dict)\n","      self.load_state_dict(model_dict)\n","      print('Model is load')\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-6sTpOoAUWZ"},"outputs":[],"source":["# Load the dataset\n","material_name = 'Material D'\n","\n","# material_b_mix   3E6_77_78_N30\n","\n","B_file_path = f'/content/drive/MyDrive/Colab_Notebooks/MagNet/Database/{material_name}/B_Field.csv'\n","# B_file_path = f'/content/drive/MyDrive/Colab_Notebooks/MagNet/Database/{material_name}/B_waveform.csv'\n","Freq_file_path = f'/content/drive/MyDrive/Colab_Notebooks/MagNet/Database/{material_name}/Frequency.csv'\n","Temp_file_path = f'/content/drive/MyDrive/Colab_Notebooks/MagNet/Database/{material_name}/Temperature.csv'\n","Power_file_path = f'/content/drive/MyDrive/Colab_Notebooks/MagNet/Database/{material_name}/Volumetric_Loss.csv'\n","\n","# Pre-Train\n","pretrain_model_path = f'/content/drive/MyDrive/Colab_Notebooks/MagNet/Final_test/FNN/Model/Test_Split/spilt622/Model_N49_optuna2_NoPretrain.sd'\n","Transfer_use = True\n","Pretrain = 'Pretrain_N49' #Pretrain/NoPretrain\n","spilt_way = 'spilt622' #spilt622/spilt2/midterm_material\n","\n","# Output\n","output_sd_path = f'/content/drive/MyDrive/Colab_Notebooks/MagNet/Final_test/FNN/Model/Test_Split/{spilt_way}/Model_{material_name}_optuna2_{Pretrain}.sd'\n","output_pred_path = f'/content/drive/MyDrive/Colab_Notebooks/MagNet/Final_test/FNN/Loss/withoutH_Transfer/{spilt_way}/pred_loss_{material_name}_optuna2_{Pretrain}.csv'\n","\n","def get_dataset():\n","\n","    B = read_csv(B_file_path)\n","    Freq = read_csv(Freq_file_path)\n","    Temp = read_csv(Temp_file_path)\n","    #H = read_csv(H_file_path)\n","    Power = read_csv(Power_file_path)\n","\n","    # Compute labels\n","    # There's approximalely an exponential relationship between Loss-Freq and Loss-Flux.\n","    # Using logarithm may help to improve the training.\n","    Freq = np.log10(Freq)\n","    Temp = np.array(Temp)\n","    Power = np.log10(Power)\n","\n","    # Reshape data\n","    Freq = torch.from_numpy(Freq).float().view(-1, 1)\n","    B = torch.from_numpy(B).float().view((-1,1024,1))\n","    #H = torch.from_numpy(H).float().view((-1,1024,1))\n","    Temp = torch.from_numpy(Temp).view(-1, 1)\n","    Power = Power.reshape((-1,1))\n","\n","    # Normalize\n","    B = (B-torch.mean(B))/torch.std(B).numpy()\n","    #H = (H-torch.mean(H))/torch.std(H).numpy()\n","    Freq = (Freq-torch.mean(Freq))/torch.std(Freq).numpy()\n","    Temp = (Temp-torch.mean(Temp))/torch.std(Temp).numpy()\n","\n","    B = np.squeeze(B, axis=2)\n","    #H = np.squeeze(H, axis=2)\n","\n","    print(np.shape(Freq))\n","    print(np.shape(B))\n","    #print(np.shape(H))\n","    print(np.shape(Temp))\n","    print(np.shape(Power))\n","\n","    temp = np.concatenate((Freq,B,Temp),axis=1)\n","\n","    in_tensors = torch.from_numpy(temp).view(-1, 1026)\n","    out_tensors = torch.from_numpy(Power).view(-1, 1)\n","\n","    return torch.utils.data.TensorDataset(in_tensors, out_tensors)\n","\n","def read_csv(file_path):\n","    data = []\n","    with open(file_path, 'r', newline='') as file:\n","        csv_reader = csv.reader(file)\n","        for row in csv_reader:\n","            values = [float(value) for value in row]\n","            data.append(values)\n","    return np.array(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21250,"status":"ok","timestamp":1703996868856,"user":{"displayName":"Yun Shan Hsieh","userId":"04540611790251984553"},"user_tz":-480},"id":"y9ndXYTq9b9R","outputId":"e46fe9e0-50cc-4b99-e492-9f5ea96d2301"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([580, 1])\n","torch.Size([580, 1024])\n","torch.Size([580, 1])\n","(580, 1)\n","Model is load\n","Number of parameters:  86728\n","Saving model with loss 129.094...\n","Saving model with loss 22.465...\n","Epoch  5 Train 49.25980 Valid 20.60413\n","Saving model with loss 20.604...\n","Saving model with loss 20.586...\n","Epoch 10 Train 15.40401 Valid 12.19422\n","Saving model with loss 12.194...\n","Saving model with loss 9.411...\n","Epoch 15 Train 6.84011 Valid 9.44941\n","Saving model with loss 7.844...\n","Saving model with loss 6.558...\n","Saving model with loss 5.932...\n","Epoch 20 Train 3.95559 Valid 5.81277\n","Saving model with loss 5.813...\n","Saving model with loss 5.577...\n","Saving model with loss 4.702...\n","Saving model with loss 4.170...\n","Epoch 25 Train 2.51129 Valid 4.10282\n","Saving model with loss 4.103...\n","Saving model with loss 4.069...\n","Saving model with loss 3.790...\n","Epoch 30 Train 1.74797 Valid 4.19225\n","Saving model with loss 3.309...\n","Epoch 35 Train 1.64718 Valid 3.43012\n","Epoch 40 Train 1.32393 Valid 3.30126\n","Saving model with loss 3.301...\n","Epoch 45 Train 1.25710 Valid 3.12527\n","Saving model with loss 3.125...\n","Saving model with loss 2.953...\n","Epoch 50 Train 1.24831 Valid 4.12541\n","Epoch 55 Train 1.99466 Valid 3.86262\n","Saving model with loss 2.735...\n","Saving model with loss 2.610...\n","Epoch 60 Train 1.52728 Valid 3.66249\n","Epoch 65 Train 1.13803 Valid 2.83202\n","Saving model with loss 2.566...\n","Epoch 70 Train 1.10275 Valid 2.79636\n","Saving model with loss 2.565...\n","Epoch 75 Train 0.83725 Valid 2.69208\n","Saving model with loss 2.453...\n","Epoch 80 Train 0.95297 Valid 2.84289\n","Epoch 85 Train 0.94669 Valid 2.57307\n","Epoch 90 Train 0.74589 Valid 2.37279\n","Saving model with loss 2.373...\n","Saving model with loss 2.284...\n","Epoch 95 Train 0.76063 Valid 3.03908\n","Epoch 100 Train 0.86942 Valid 2.93136\n","Saving model with loss 2.196...\n","Epoch 105 Train 0.81003 Valid 2.97156\n","Saving model with loss 2.055...\n","Epoch 110 Train 0.66039 Valid 2.59422\n","Saving model with loss 2.016...\n","Epoch 115 Train 0.75487 Valid 2.56544\n","Epoch 120 Train 1.13420 Valid 2.28070\n","Epoch 125 Train 1.15369 Valid 2.98007\n","Epoch 130 Train 0.68973 Valid 2.09242\n","Epoch 135 Train 0.92404 Valid 2.04286\n","Epoch 140 Train 0.91685 Valid 2.01233\n","Saving model with loss 2.012...\n","Saving model with loss 2.008...\n","Epoch 145 Train 1.34727 Valid 3.00559\n","Epoch 150 Train 0.56323 Valid 2.08266\n","Epoch 155 Train 1.07350 Valid 2.09571\n","Epoch 160 Train 0.58920 Valid 2.09180\n","Saving model with loss 1.945...\n","Epoch 165 Train 0.70964 Valid 1.99168\n","Epoch 170 Train 1.05366 Valid 3.02429\n","Epoch 175 Train 0.64965 Valid 2.49784\n","Epoch 180 Train 0.53655 Valid 1.92153\n","Saving model with loss 1.922...\n","Epoch 185 Train 0.52339 Valid 1.96396\n","Epoch 190 Train 0.29247 Valid 2.03079\n","Epoch 195 Train 0.25851 Valid 2.01588\n","Epoch 200 Train 0.30597 Valid 2.26664\n","Epoch 205 Train 0.46527 Valid 2.62471\n","Epoch 210 Train 0.36691 Valid 2.06017\n","Epoch 215 Train 0.53897 Valid 2.17996\n","Epoch 220 Train 0.67471 Valid 3.42316\n","Epoch 225 Train 0.95715 Valid 2.53376\n","Epoch 230 Train 3.28764 Valid 4.83687\n","Epoch 235 Train 2.72829 Valid 4.90231\n","Epoch 240 Train 3.85017 Valid 3.99845\n","Epoch 245 Train 2.44587 Valid 3.31000\n","Epoch 250 Train 2.73860 Valid 5.53304\n","Epoch 255 Train 2.68110 Valid 3.49196\n","Epoch 260 Train 1.63761 Valid 2.47331\n","Epoch 265 Train 1.34844 Valid 2.37795\n","Epoch 270 Train 1.45359 Valid 3.39411\n","Saving model with loss 1.904...\n","Epoch 275 Train 1.41652 Valid 2.56571\n","Epoch 280 Train 2.05257 Valid 4.02044\n","Saving model with loss 1.847...\n","Epoch 285 Train 0.89171 Valid 3.15898\n","Epoch 290 Train 0.88326 Valid 2.36394\n","Saving model with loss 1.597...\n","Epoch 295 Train 0.62454 Valid 2.34537\n","Epoch 300 Train 0.52782 Valid 2.19744\n","Epoch 305 Train 0.55818 Valid 2.33324\n","Epoch 310 Train 0.90479 Valid 1.96469\n","Epoch 315 Train 0.80222 Valid 1.85255\n","Epoch 320 Train 0.68369 Valid 2.10055\n","Saving model with loss 1.511...\n","Epoch 325 Train 0.49233 Valid 1.98560\n","Epoch 330 Train 0.51191 Valid 1.77986\n","Epoch 335 Train 0.39199 Valid 2.00005\n","Epoch 340 Train 0.45259 Valid 1.68827\n","Epoch 345 Train 0.40455 Valid 1.83387\n","Epoch 350 Train 0.27016 Valid 1.54204\n","Epoch 355 Train 0.41893 Valid 1.95112\n","Epoch 360 Train 0.81434 Valid 1.69461\n","Epoch 365 Train 0.73516 Valid 2.55921\n","Epoch 370 Train 0.46019 Valid 2.13780\n","Epoch 375 Train 0.36648 Valid 1.78311\n","Epoch 380 Train 0.31103 Valid 1.83880\n","Epoch 385 Train 1.54448 Valid 2.19728\n","Epoch 390 Train 0.52344 Valid 1.73263\n","Epoch 395 Train 0.39095 Valid 1.94543\n","Epoch 400 Train 0.60358 Valid 2.34529\n","Epoch 405 Train 3.98143 Valid 4.25820\n","Epoch 410 Train 2.40524 Valid 3.17505\n","Epoch 415 Train 1.14269 Valid 1.94432\n","Epoch 420 Train 1.66204 Valid 2.56907\n","Epoch 425 Train 0.70970 Valid 1.65733\n","Epoch 430 Train 0.23173 Valid 1.63259\n","Saving model with loss 1.461...\n","Saving model with loss 1.452...\n","Epoch 435 Train 0.15977 Valid 1.51408\n","Epoch 440 Train 0.13343 Valid 1.49065\n","Saving model with loss 1.448...\n","Epoch 445 Train 0.10654 Valid 1.44975\n","Epoch 450 Train 0.09511 Valid 1.47510\n","Epoch 455 Train 0.09913 Valid 1.50361\n","Saving model with loss 1.443...\n","Epoch 460 Train 0.09115 Valid 1.46497\n","Epoch 465 Train 0.08386 Valid 1.47887\n","Epoch 470 Train 0.08179 Valid 1.49622\n","Epoch 475 Train 0.08039 Valid 1.46547\n","Epoch 480 Train 0.06831 Valid 1.50242\n","Epoch 485 Train 0.07751 Valid 1.48718\n","Epoch 490 Train 0.05844 Valid 1.50254\n","Epoch 495 Train 0.06365 Valid 1.48391\n","Epoch 500 Train 0.06949 Valid 1.49502\n","Epoch 505 Train 0.05845 Valid 1.50823\n","Epoch 510 Train 0.06181 Valid 1.49355\n","Epoch 515 Train 0.05525 Valid 1.50048\n","Epoch 520 Train 0.06480 Valid 1.52990\n","Epoch 525 Train 0.06423 Valid 1.53324\n","Epoch 530 Train 0.05252 Valid 1.53138\n","Epoch 535 Train 0.07492 Valid 1.64584\n","Epoch 540 Train 0.08391 Valid 1.49492\n","Epoch 545 Train 0.07695 Valid 1.65842\n","Epoch 550 Train 0.05222 Valid 1.54858\n","Epoch 555 Train 0.04968 Valid 1.55330\n","Epoch 560 Train 0.06043 Valid 1.61972\n","Epoch 565 Train 0.05577 Valid 1.59949\n","Epoch 570 Train 0.07532 Valid 1.67927\n","Epoch 575 Train 0.05409 Valid 1.56134\n","Epoch 580 Train 0.05274 Valid 1.60764\n","Epoch 585 Train 0.05726 Valid 1.64033\n","Epoch 590 Train 0.04687 Valid 1.59792\n","Epoch 595 Train 0.04203 Valid 1.60332\n","Epoch 600 Train 0.04889 Valid 1.52698\n","Epoch 605 Train 0.05052 Valid 1.58137\n","Epoch 610 Train 0.06399 Valid 1.69213\n","Epoch 615 Train 0.05318 Valid 1.58237\n","Epoch 620 Train 0.06409 Valid 1.61658\n","Epoch 625 Train 0.04792 Valid 1.68001\n","Epoch 630 Train 0.04254 Valid 1.61316\n","Epoch 635 Train 0.03912 Valid 1.59835\n","Epoch 640 Train 0.04470 Valid 1.62510\n","Epoch 645 Train 0.04930 Valid 1.62390\n","Epoch 650 Train 0.04020 Valid 1.64505\n","Epoch 655 Train 0.03444 Valid 1.59184\n","Epoch 660 Train 0.05596 Valid 1.57214\n","Epoch 665 Train 0.04961 Valid 1.68537\n","Epoch 670 Train 0.05455 Valid 1.61567\n","Epoch 675 Train 0.03529 Valid 1.66922\n","Epoch 680 Train 0.04407 Valid 1.64065\n","Epoch 685 Train 0.05349 Valid 1.75729\n","Epoch 690 Train 0.08918 Valid 1.63944\n","Epoch 695 Train 0.24330 Valid 2.11792\n","Epoch 700 Train 0.12414 Valid 1.83419\n","Epoch 705 Train 0.18237 Valid 1.98283\n","Epoch 710 Train 0.18307 Valid 1.87087\n","Epoch 715 Train 0.27014 Valid 2.02862\n","Epoch 720 Train 0.11557 Valid 1.69840\n","Epoch 725 Train 0.13298 Valid 1.72234\n","Epoch 730 Train 0.07528 Valid 1.64173\n","Epoch 735 Train 0.05461 Valid 1.70527\n","Epoch 740 Train 0.05072 Valid 1.67167\n","Epoch 745 Train 0.03575 Valid 1.59354\n","Epoch 750 Train 0.03602 Valid 1.67259\n","Epoch 755 Train 0.03324 Valid 1.68387\n","Epoch 760 Train 0.05062 Valid 1.74234\n","Epoch 765 Train 0.03914 Valid 1.73409\n","Epoch 770 Train 0.10131 Valid 1.63389\n","Epoch 775 Train 0.05011 Valid 1.87521\n","Epoch 780 Train 0.17299 Valid 1.61194\n","Epoch 785 Train 0.06789 Valid 1.63015\n","Epoch 790 Train 0.18188 Valid 1.76885\n","Epoch 795 Train 0.17001 Valid 2.09734\n","Epoch 800 Train 0.14443 Valid 1.85476\n","Epoch 805 Train 0.08541 Valid 1.64193\n","Epoch 810 Train 0.11301 Valid 1.63750\n","Epoch 815 Train 0.13724 Valid 1.83950\n","Epoch 820 Train 0.04586 Valid 1.64263\n","Epoch 825 Train 0.04899 Valid 1.59013\n","Epoch 830 Train 0.12761 Valid 1.75364\n","Epoch 835 Train 0.07100 Valid 1.74428\n","Epoch 840 Train 0.08874 Valid 1.83371\n","Epoch 845 Train 0.64758 Valid 2.50133\n","Epoch 850 Train 0.24585 Valid 1.76373\n","Epoch 855 Train 0.09897 Valid 1.72183\n","Epoch 860 Train 0.03849 Valid 1.65656\n","Epoch 865 Train 0.02932 Valid 1.64643\n","Epoch 870 Train 0.02292 Valid 1.62623\n","Epoch 875 Train 0.02276 Valid 1.63932\n","Epoch 880 Train 0.02360 Valid 1.65665\n","Epoch 885 Train 0.02298 Valid 1.67153\n","Epoch 890 Train 0.02257 Valid 1.65797\n","Epoch 895 Train 0.02373 Valid 1.65001\n","Epoch 900 Train 0.03196 Valid 1.67993\n","Epoch 905 Train 0.02372 Valid 1.65321\n","Epoch 910 Train 0.02131 Valid 1.67474\n","Epoch 915 Train 0.02120 Valid 1.65114\n","Epoch 920 Train 0.01977 Valid 1.66817\n","Epoch 925 Train 0.02167 Valid 1.66876\n","Epoch 930 Train 0.02051 Valid 1.67904\n","Epoch 935 Train 0.02257 Valid 1.67957\n","Epoch 940 Train 0.02004 Valid 1.68407\n","Epoch 945 Train 0.02980 Valid 1.66169\n","Epoch 950 Train 0.01771 Valid 1.67823\n","Epoch 955 Train 0.01874 Valid 1.68515\n","Model is not improving, so we halt the training session.\n","Training finished! Model is saved!\n","Best model is load to test\n","Test Loss: 2.26840\n","yy_pred : (116, 1)\n","yy_meas : (116, 1)\n","Relative Error: 7.46986580\n","AVG Error: 7.46986580\n","95-PRCT Error: 19.97739663\n","99th Percentile Error: 37.74981916\n","MAX Error: 46.74637394\n"]}],"source":["# Config the model training\n","\n","def main():\n","    # Reproducibility\n","    random.seed(1)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Hyperparameters\n","    NUM_EPOCH = 2000\n","    BATCH_SIZE = 128\n","    DECAY_EPOCH = 423\n","    DECAY_RATIO = 0.458\n","    LR_INI = 0.005\n","    best_loss = math.inf\n","    early_stop_count = 0\n","    early_stop = 500\n","    # Select GPU as default device\n","    device = torch.device(\"cuda\")\n","\n","    # Load dataset\n","    dataset = get_dataset()\n","\n","    # Split the dataset\n","    train_size = int(0.6 * len(dataset))\n","    valid_size = int(0.2 * len(dataset))\n","    test_size = len(dataset) - train_size - valid_size\n","    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n","    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n","    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n","\n","    # Setup network\n","    # net = Net().double().to(device)\n","    net = Net(Transfer_use ,pretrain_model_path).double().to(device)\n","\n","    # Log the number of parameters\n","    print(\"Number of parameters: \", count_parameters(net))\n","\n","    # Setup optimizer\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(net.parameters(), lr=LR_INI)\n","\n","    # Train the network\n","    for epoch_i in range(NUM_EPOCH):\n","\n","        # Train for one epoch\n","        epoch_train_loss = 0\n","        net.train()\n","        optimizer.param_groups[0]['lr'] = LR_INI* (DECAY_RATIO ** (0+ epoch_i // DECAY_EPOCH))\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = net(inputs.to(device))\n","            loss = criterion(outputs, labels.to(device))\n","            loss.backward()\n","            optimizer.step()\n","            epoch_train_loss += loss.item()\n","\n","        # Compute Validation Loss\n","        with torch.no_grad():\n","            epoch_valid_loss = 0\n","            for inputs, labels in valid_loader:\n","                outputs = net(inputs.to(device))\n","                loss = criterion(outputs, labels.to(device))\n","\n","                epoch_valid_loss += loss.item()\n","\n","        if (epoch_i+1)%5 == 0:\n","          print(f\"Epoch {epoch_i+1:2d} \"\n","              f\"Train {epoch_train_loss / len(train_dataset) * 1e5:.5f} \"\n","              f\"Valid {epoch_valid_loss / len(valid_dataset) * 1e5:.5f}\")\n","\n","        # Early stop\n","        epoch_valid_loss = epoch_valid_loss / len(valid_dataset) * 1e5\n","        if epoch_valid_loss < best_loss:\n","          best_loss = epoch_valid_loss\n","          torch.save(net.state_dict(), output_sd_path)  # Save your best model\n","          print('Saving model with loss {:.3f}...'.format(best_loss))\n","          early_stop_count = 0\n","        else:\n","          early_stop_count += 1\n","\n","        if early_stop_count >= early_stop:\n","          print('Model is not improving, so we halt the training session.')\n","          break\n","    print(\"Training finished! Model is saved!\")\n","\n","    # Load the best model  ====================================================\n","    net.load_state_dict(torch.load(output_sd_path))\n","    print(\"Best model is load to test\")\n","    # =====================================================================\n","\n","    # Evaluation\n","    net.eval()\n","    y_meas = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            y_pred.append(net(inputs.to(device)))\n","            y_meas.append(labels.to(device))\n","\n","    y_meas = torch.cat(y_meas, dim=0)\n","    y_pred = torch.cat(y_pred, dim=0)\n","    print(f\"Test Loss: {F.mse_loss(y_meas, y_pred).item() / len(test_dataset) * 1e5:.5f}\")\n","\n","    yy_pred = 10**(y_pred.cpu().numpy())\n","    yy_meas = 10**(y_meas.cpu().numpy())\n","    print(\"yy_pred :\", yy_pred.shape)\n","    print(\"yy_meas :\", yy_meas.shape)\n","\n","    with open(output_pred_path, \"w\") as f:\n","        np.savetxt(f, (yy_pred))\n","        f.close()\n","    #with open(\"/content/drive/MyDrive/材料/Material B/Pred_loss/meas_loss_Material B_optuna3.csv\", \"w\") as f:\n","        #np.savetxt(f, (yy_meas))\n","        #f.close()\n","\n","    # Relative Error\n","    Error_re = abs(yy_pred-yy_meas)/abs(yy_meas)*100\n","    Error_re_avg = np.mean(Error_re)\n","    Error_re_rms = np.sqrt(np.mean(Error_re ** 2))\n","    Error_re_95prct = np.percentile(Error_re, 95)\n","    Error_re_99prct = np.percentile(Error_re, 99)\n","    Error_re_max = np.max(Error_re)\n","\n","    print(f\"Relative Error: {Error_re_avg:.8f}\")\n","    print(f\"AVG Error: {Error_re_avg:.8f}\")\n","    # print(f\"RMS Error: {Error_re_rms:.8f}\")\n","    print(f\"95-PRCT Error: {Error_re_95prct:.8f}\")\n","    print(f\"99th Percentile Error: {Error_re_99prct:.8f}\")\n","    print(f\"MAX Error: {Error_re_max:.8f}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
