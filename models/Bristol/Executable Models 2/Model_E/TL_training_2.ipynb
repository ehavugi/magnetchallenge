{"cells":[{"cell_type":"markdown","metadata":{"id":"XQSzNovNpM1J"},"source":["# User Colab Path Setting"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":280,"status":"ok","timestamp":1703794261453,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"BY47VVFEpe5q"},"outputs":[],"source":["colab_dir = '/content/drive/MyDrive/DeepLearning/Grp_Small/TL/MagNet_small_modelE_cycle'  # example for colab\n","\n","platform = 'auto' # auto detect platform (colab, windows_local, linux_local, unknown)\n","#platform = 'colab'\n","#platform = 'windows_local'\n","#platform = 'linux_local'\n","#platform = 'unknown'"]},{"cell_type":"markdown","metadata":{"id":"EN1rAR3ADcfK"},"source":["# Trainnig process"]},{"cell_type":"markdown","metadata":{"id":"Ey9byKn_DcfK"},"source":["### Defult path config"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703794261861,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"F5fy2h1NDcfK"},"outputs":[],"source":["model_saved_name=\"model_tl.ckpt\"\n","dataset_path=\"data/tl_dataset\""]},{"cell_type":"markdown","metadata":{"id":"F3jSpmi4pU5n"},"source":["### Path config"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1876,"status":"ok","timestamp":1703794263734,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"7EAuEhFiocKq","outputId":"b2016227-3687-411e-c9f3-1ae13f199ff6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","current execution path:  /content/drive/MyDrive/DeepLearning/Grp_Small/TL/MagNet_small_modelE_cycle\n","\n","current platform:  colab\n"]}],"source":["import os\n","\n","try:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","except ImportError:\n","    if os.path.exists('c:/'):  # check if it is windows\n","        platform = 'windows_local'\n","    elif os.path.exists('/home/'):  # check if it is linux\n","        platform = 'linux_local'\n","    else:\n","        platform = 'unknown'\n","else:\n","    platform = 'colab'\n","\n","if platform == 'colab':\n","  os.chdir(colab_dir)\n","\n","print('\\ncurrent execution path: ', os.getcwd())  #获取当前工作目录路径\n","print('\\ncurrent platform: ', platform)  #获取当前工作目录路径"]},{"cell_type":"markdown","metadata":{"id":"CHDhat43ogMQ"},"source":["## Cuda check"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1703794263734,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"7Aipe8RLOzQk","outputId":"58da03c6-da95-4b8e-8629-4cf3fb14589b"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda good!\n","GPU num:  1\n","GPU type:  Tesla V100-SXM2-16GB\n","GPU memory: 16.94 Gbyte\n"]}],"source":["import torch\n","\n","gpu_num = 0\n","cuda_ready = False\n","\n","if torch.cuda.is_available():\n","    cuda_ready = True\n","    print('cuda good!')\n","    gpu_num = torch.cuda.device_count()\n","    if (gpu_num < 1):\n","        print('GPU unavailable')\n","    else:\n","        print('GPU num: ', gpu_num)  # 查看GPU数量\n","        for gpu in range(gpu_num):\n","            print('GPU type: ', torch.cuda.get_device_name(gpu))  # 查看GPU名称\n","            print('GPU memory: {:.2f} Gbyte'.format(\n","                torch.cuda.get_device_properties(gpu).total_memory /\n","                1e9))  # 查看GPU总内存\n","else:\n","    cuda_ready = False\n","    print('cuda unavailable!')\n"]},{"cell_type":"markdown","metadata":{"id":"9z1whOUtaPbq"},"source":["## Start coding"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1703794263735,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"LSndganfafmO","outputId":"8756cf11-1b1f-4531-c55d-0a36f6eea042"},"outputs":[{"output_type":"stream","name":"stdout","text":["colab\n","/content/drive/MyDrive/DeepLearning/Grp_Small/TL/MagNet_small_modelE_cycle\n","True\n","/content/drive/MyDrive/DeepLearning/Grp_Small/TL/MagNet_small_modelE_cycle\n"]}],"source":["print(platform)\n","print(os.getcwd())\n","print(cuda_ready)\n","print(os.path.abspath(''))"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703794263735,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"8q3WkMaUtvSZ"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","\n","import NW_LSTM\n","import NN_DataLoader"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1703794263735,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"ftzXEac3tyHc","outputId":"a85e55f0-b2f7-4679-c415-d5a13512f421"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device using  cuda\n","LSTMSeq2One(\n","  (lstm): LSTM(3, 30, num_layers=2, batch_first=True)\n","  (fc1): Linear(in_features=30, out_features=64, bias=True)\n","  (fc2): Linear(in_features=64, out_features=32, bias=True)\n","  (fc3): Linear(in_features=32, out_features=16, bias=True)\n","  (fc4): Linear(in_features=16, out_features=8, bias=True)\n","  (fc5): Linear(in_features=8, out_features=8, bias=True)\n","  (fc6): Linear(in_features=8, out_features=1, bias=True)\n","  (relu): ReLU()\n","  (leaky_relu): LeakyReLU(negative_slope=0.01)\n","  (elu): ELU(alpha=1.0)\n",")\n","Total number of parameters:  16449\n","Pre-train model loaded\n"]}],"source":["# Check if CUDA is available and if so, set the device to GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","print(\"Device using \",device)\n","\n","# Instantiate the model with appropriate dimensions\n","model = model = NW_LSTM.get_global_model().to(device)\n","\n","# Print the model architecture and parameters number\n","print(model)\n","print(\"Total number of parameters: \", sum(p.numel() for p in model.parameters()))\n","\n","# Load the pre-train model if it exists\n","try:\n","    model.load_state_dict(torch.load(model_saved_name))\n","    print(\"Pre-train model loaded\")\n","except:\n","    print(\"No model found, start training from scratch\")\n","    pass\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QKhTKu9cDcfN"},"source":["### Define training para"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1703794263735,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"X6OqVZExt06B"},"outputs":[],"source":["def train_model(epoch_num=700,lr=2e-4,method=\"forward\"):\n","\n","    # Define the loss function and optimizer\n","    #loss_fn = nn.MSELoss()\n","    loss_fn = NW_LSTM.RelativeLoss()\n","    #loss_fn = NW_LSTM.RelativeLoss_abs()\n","    optimizer = optim.AdamW(model.parameters(), lr=lr)\n","\n","    # lr scheduler\n","    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=1, last_epoch=-1)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200, eta_min=0, last_epoch=-1)\n","\n","    # Default para in desktop env\n","    epochs = 10\n","    valid_batch_size=1000\n","\n","    if platform == \"colab\":\n","      epochs = epoch_num\n","      valid_batch_size=3000\n","\n","\n","    train_dataloader = NN_DataLoader.get_dataLoader(os.path.normpath(dataset_path +\n","                                                                \"/train.mat\"),\n","                                            batch_size=128)\n","\n","    # Get validation data\n","    valid_dataloader = NN_DataLoader.get_dataLoader(os.path.normpath(dataset_path +\n","                                                                \"/valid.mat\"),\n","                                                batch_size=valid_batch_size)\n","    valid_inputs, valid_targets = next(iter(valid_dataloader))\n","    valid_inputs, valid_targets = valid_inputs.to(device), valid_targets.to(device)\n","\n","\n","\n","    # estimate time used for training\n","    import time\n","    t0 = time.perf_counter()\n","\n","    # Save the model with the lowest validation loss\n","    with torch.no_grad():\n","        valid_outputs = model(valid_inputs)\n","        # Compute loss\n","        minium_loss = loss_fn(valid_outputs, valid_targets)\n","\n","    # Train the model\n","    for epoch in range(epochs):\n","\n","\n","        # estimate time used for one epoch(s)\n","        t_epoch = time.perf_counter() - t0\n","        t0 = time.perf_counter()\n","\n","        # Train one epoch\n","        for i, (train_inputs, train_targets) in enumerate(train_dataloader):\n","            # Move data to device\n","            train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n","\n","            # Forward pass\n","            if method == \"forward\":\n","                train_outputs = model(train_inputs)\n","            elif method == \"valid\":\n","                train_outputs = model.valid(train_inputs)\n","\n","            # Compute loss\n","            loss = loss_fn(train_outputs, train_targets)\n","\n","            # Backward pass and optimize\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Compute validation loss\n","        if epoch > 0:\n","            with torch.no_grad():\n","                valid_outputs = model(valid_inputs)\n","                # Compute loss\n","                valid_loss = loss_fn(valid_outputs, valid_targets)\n","\n","            if valid_loss < minium_loss:\n","                minium_loss = valid_loss\n","                torch.save(model.state_dict(), model_saved_name)\n","                print(f\"  Model saved , Validation Loss: {valid_loss.item():.3e}, lr: {optimizer.param_groups[0]['lr']:.3e}\")\n","\n","        # update lr\n","        scheduler.step()\n","\n","        # Print loss every 10 epochs\n","        if (epoch + 1) % 10 == 0:\n","            print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {loss.item():.3e}, \"\n","                #   f\"Validation Loss: {valid_loss.item():.3e} ,\"\n","                f\"Remain time: {t_epoch/60 * (epochs - epoch - 1):.1f} min\")\n"]},{"cell_type":"markdown","metadata":{"id":"JVWF5osJDcfN"},"source":["### Training loop"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1703794263735,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"HVfNZrtoottv"},"outputs":[],"source":["#train_model(epoch_num=10,lr=2e-4,method=\"forward\")"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1060289,"status":"ok","timestamp":1703795324020,"user":{"displayName":"Lizhong Zhang","userId":"15095891074797676359"},"user_tz":0},"id":"oGiB5uWYottv","outputId":"4d2d20b0-dbec-404b-bd32-dacef5541067"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Model saved , Validation Loss: 2.106e-03, lr: 9.999e-05\n","  Model saved , Validation Loss: 1.765e-03, lr: 9.990e-05\n","Epoch 10/3000, Training Loss: 1.631e-03, Remain time: 15.9 min\n","Epoch 20/3000, Training Loss: 1.366e-03, Remain time: 15.4 min\n","Epoch 30/3000, Training Loss: 1.220e-03, Remain time: 15.2 min\n","Epoch 40/3000, Training Loss: 1.675e-03, Remain time: 23.5 min\n","Epoch 50/3000, Training Loss: 2.373e-03, Remain time: 15.0 min\n","Epoch 60/3000, Training Loss: 1.033e-03, Remain time: 16.1 min\n","Epoch 70/3000, Training Loss: 1.405e-03, Remain time: 15.5 min\n","Epoch 80/3000, Training Loss: 1.299e-03, Remain time: 19.7 min\n","Epoch 90/3000, Training Loss: 9.782e-04, Remain time: 22.8 min\n","Epoch 100/3000, Training Loss: 1.249e-03, Remain time: 15.9 min\n","Epoch 110/3000, Training Loss: 1.109e-03, Remain time: 15.3 min\n","Epoch 120/3000, Training Loss: 1.035e-03, Remain time: 14.3 min\n","Epoch 130/3000, Training Loss: 1.493e-03, Remain time: 20.3 min\n","Epoch 140/3000, Training Loss: 1.023e-03, Remain time: 14.0 min\n","Epoch 150/3000, Training Loss: 1.429e-03, Remain time: 14.6 min\n","Epoch 160/3000, Training Loss: 1.188e-03, Remain time: 14.3 min\n","Epoch 170/3000, Training Loss: 1.140e-03, Remain time: 20.2 min\n","Epoch 180/3000, Training Loss: 1.357e-03, Remain time: 14.9 min\n","Epoch 190/3000, Training Loss: 1.468e-03, Remain time: 14.2 min\n","Epoch 200/3000, Training Loss: 1.201e-03, Remain time: 14.9 min\n","Epoch 210/3000, Training Loss: 8.593e-04, Remain time: 21.5 min\n","Epoch 220/3000, Training Loss: 1.103e-03, Remain time: 14.4 min\n","Epoch 230/3000, Training Loss: 9.192e-04, Remain time: 13.8 min\n","Epoch 240/3000, Training Loss: 1.656e-03, Remain time: 13.7 min\n","Epoch 250/3000, Training Loss: 9.864e-04, Remain time: 15.0 min\n","Epoch 260/3000, Training Loss: 1.286e-03, Remain time: 19.9 min\n","Epoch 270/3000, Training Loss: 1.090e-03, Remain time: 13.8 min\n","Epoch 280/3000, Training Loss: 1.456e-03, Remain time: 14.3 min\n","Epoch 290/3000, Training Loss: 1.398e-03, Remain time: 13.5 min\n","Epoch 300/3000, Training Loss: 1.385e-03, Remain time: 18.9 min\n","Epoch 310/3000, Training Loss: 1.031e-03, Remain time: 14.6 min\n","Epoch 320/3000, Training Loss: 7.490e-04, Remain time: 14.4 min\n","Epoch 330/3000, Training Loss: 1.308e-03, Remain time: 19.0 min\n","Epoch 340/3000, Training Loss: 1.132e-03, Remain time: 19.1 min\n","Epoch 350/3000, Training Loss: 1.199e-03, Remain time: 13.3 min\n","Epoch 360/3000, Training Loss: 1.038e-03, Remain time: 13.4 min\n","Epoch 370/3000, Training Loss: 1.569e-03, Remain time: 13.3 min\n","Epoch 380/3000, Training Loss: 1.206e-03, Remain time: 18.5 min\n","Epoch 390/3000, Training Loss: 1.372e-03, Remain time: 13.3 min\n","  Model saved , Validation Loss: 1.735e-03, lr: 9.998e-05\n","Epoch 400/3000, Training Loss: 1.712e-03, Remain time: 14.6 min\n","Epoch 410/3000, Training Loss: 1.456e-03, Remain time: 13.7 min\n","Epoch 420/3000, Training Loss: 1.065e-03, Remain time: 18.2 min\n","Epoch 430/3000, Training Loss: 1.163e-03, Remain time: 15.2 min\n","Epoch 440/3000, Training Loss: 1.281e-03, Remain time: 12.9 min\n","Epoch 450/3000, Training Loss: 1.022e-03, Remain time: 12.7 min\n","Epoch 460/3000, Training Loss: 9.275e-04, Remain time: 12.7 min\n","Epoch 470/3000, Training Loss: 2.073e-03, Remain time: 18.5 min\n","Epoch 480/3000, Training Loss: 1.154e-03, Remain time: 12.7 min\n","Epoch 490/3000, Training Loss: 9.597e-04, Remain time: 12.6 min\n","Epoch 500/3000, Training Loss: 1.255e-03, Remain time: 13.5 min\n","Epoch 510/3000, Training Loss: 1.398e-03, Remain time: 18.3 min\n","Epoch 520/3000, Training Loss: 1.128e-03, Remain time: 12.7 min\n","Epoch 530/3000, Training Loss: 1.219e-03, Remain time: 12.4 min\n","Epoch 540/3000, Training Loss: 9.012e-04, Remain time: 13.6 min\n","Epoch 550/3000, Training Loss: 1.630e-03, Remain time: 18.0 min\n","Epoch 560/3000, Training Loss: 1.146e-03, Remain time: 12.7 min\n","Epoch 570/3000, Training Loss: 1.206e-03, Remain time: 13.0 min\n","Epoch 580/3000, Training Loss: 1.461e-03, Remain time: 13.3 min\n","Epoch 590/3000, Training Loss: 1.058e-03, Remain time: 19.1 min\n","Epoch 600/3000, Training Loss: 1.021e-03, Remain time: 12.6 min\n","Epoch 610/3000, Training Loss: 1.159e-03, Remain time: 13.0 min\n","Epoch 620/3000, Training Loss: 9.092e-04, Remain time: 12.2 min\n","  Model saved , Validation Loss: 1.735e-03, lr: 4.430e-06\n","Epoch 630/3000, Training Loss: 1.443e-03, Remain time: 16.1 min\n","Epoch 640/3000, Training Loss: 1.067e-03, Remain time: 18.8 min\n","Epoch 650/3000, Training Loss: 1.365e-03, Remain time: 12.3 min\n","Epoch 660/3000, Training Loss: 1.049e-03, Remain time: 12.5 min\n","Epoch 670/3000, Training Loss: 9.483e-04, Remain time: 12.5 min\n","Epoch 680/3000, Training Loss: 1.126e-03, Remain time: 18.6 min\n","Epoch 690/3000, Training Loss: 9.408e-04, Remain time: 12.5 min\n","Epoch 700/3000, Training Loss: 1.194e-03, Remain time: 12.6 min\n","Epoch 710/3000, Training Loss: 1.565e-03, Remain time: 12.5 min\n","Epoch 720/3000, Training Loss: 1.857e-03, Remain time: 17.7 min\n","  Model saved , Validation Loss: 1.649e-03, lr: 6.841e-05\n","Epoch 730/3000, Training Loss: 7.040e-04, Remain time: 12.0 min\n","Epoch 740/3000, Training Loss: 1.511e-03, Remain time: 12.5 min\n","Epoch 750/3000, Training Loss: 1.320e-03, Remain time: 12.3 min\n","Epoch 760/3000, Training Loss: 1.240e-03, Remain time: 18.0 min\n","Epoch 770/3000, Training Loss: 1.376e-03, Remain time: 11.9 min\n","Epoch 780/3000, Training Loss: 1.169e-03, Remain time: 12.1 min\n","Epoch 790/3000, Training Loss: 1.278e-03, Remain time: 12.1 min\n","Epoch 800/3000, Training Loss: 8.216e-04, Remain time: 16.7 min\n","Epoch 810/3000, Training Loss: 1.755e-03, Remain time: 11.6 min\n","Epoch 820/3000, Training Loss: 1.654e-03, Remain time: 11.7 min\n","Epoch 830/3000, Training Loss: 1.314e-03, Remain time: 12.0 min\n","Epoch 840/3000, Training Loss: 1.106e-03, Remain time: 15.7 min\n","Epoch 850/3000, Training Loss: 9.155e-04, Remain time: 11.5 min\n","Epoch 860/3000, Training Loss: 1.113e-03, Remain time: 12.2 min\n","Epoch 870/3000, Training Loss: 1.284e-03, Remain time: 12.4 min\n","Epoch 880/3000, Training Loss: 1.144e-03, Remain time: 16.5 min\n","Epoch 890/3000, Training Loss: 1.316e-03, Remain time: 11.6 min\n","Epoch 900/3000, Training Loss: 9.516e-04, Remain time: 11.9 min\n","Epoch 910/3000, Training Loss: 1.269e-03, Remain time: 11.4 min\n","Epoch 920/3000, Training Loss: 1.382e-03, Remain time: 16.8 min\n","Epoch 930/3000, Training Loss: 1.036e-03, Remain time: 12.0 min\n","Epoch 940/3000, Training Loss: 1.197e-03, Remain time: 11.0 min\n","Epoch 950/3000, Training Loss: 1.416e-03, Remain time: 11.3 min\n","Epoch 960/3000, Training Loss: 1.054e-03, Remain time: 15.7 min\n","Epoch 970/3000, Training Loss: 1.146e-03, Remain time: 11.4 min\n","Epoch 980/3000, Training Loss: 7.122e-04, Remain time: 11.1 min\n","Epoch 990/3000, Training Loss: 1.050e-03, Remain time: 11.6 min\n","Epoch 1000/3000, Training Loss: 1.006e-03, Remain time: 16.0 min\n","Epoch 1010/3000, Training Loss: 1.493e-03, Remain time: 10.8 min\n","Epoch 1020/3000, Training Loss: 1.268e-03, Remain time: 11.0 min\n","  Model saved , Validation Loss: 1.635e-03, lr: 4.759e-06\n","Epoch 1030/3000, Training Loss: 6.275e-04, Remain time: 10.7 min\n","Epoch 1040/3000, Training Loss: 1.086e-03, Remain time: 15.8 min\n","Epoch 1050/3000, Training Loss: 9.685e-04, Remain time: 10.8 min\n","Epoch 1060/3000, Training Loss: 9.518e-04, Remain time: 10.7 min\n","Epoch 1070/3000, Training Loss: 9.939e-04, Remain time: 10.1 min\n","Epoch 1080/3000, Training Loss: 1.700e-03, Remain time: 14.6 min\n","Epoch 1090/3000, Training Loss: 7.569e-04, Remain time: 10.1 min\n","Epoch 1100/3000, Training Loss: 1.737e-03, Remain time: 10.6 min\n","Epoch 1110/3000, Training Loss: 1.339e-03, Remain time: 10.4 min\n","Epoch 1120/3000, Training Loss: 1.138e-03, Remain time: 14.6 min\n","Epoch 1130/3000, Training Loss: 1.597e-03, Remain time: 9.7 min\n","Epoch 1140/3000, Training Loss: 5.951e-04, Remain time: 9.8 min\n","Epoch 1150/3000, Training Loss: 1.065e-03, Remain time: 10.3 min\n","Epoch 1160/3000, Training Loss: 1.166e-03, Remain time: 14.3 min\n","Epoch 1170/3000, Training Loss: 1.370e-03, Remain time: 10.9 min\n","Epoch 1180/3000, Training Loss: 1.347e-03, Remain time: 9.7 min\n","Epoch 1190/3000, Training Loss: 8.333e-04, Remain time: 9.5 min\n","Epoch 1200/3000, Training Loss: 1.105e-03, Remain time: 9.5 min\n","Epoch 1210/3000, Training Loss: 1.410e-03, Remain time: 14.7 min\n","Epoch 1220/3000, Training Loss: 1.186e-03, Remain time: 9.7 min\n","Epoch 1230/3000, Training Loss: 1.285e-03, Remain time: 10.0 min\n","Epoch 1240/3000, Training Loss: 1.151e-03, Remain time: 9.5 min\n","Epoch 1250/3000, Training Loss: 8.021e-04, Remain time: 14.5 min\n","Epoch 1260/3000, Training Loss: 1.183e-03, Remain time: 9.8 min\n","Epoch 1270/3000, Training Loss: 1.024e-03, Remain time: 9.5 min\n","Epoch 1280/3000, Training Loss: 6.912e-04, Remain time: 9.8 min\n","Epoch 1290/3000, Training Loss: 1.894e-03, Remain time: 13.8 min\n","Epoch 1300/3000, Training Loss: 1.433e-03, Remain time: 9.2 min\n","Epoch 1310/3000, Training Loss: 1.070e-03, Remain time: 9.3 min\n","Epoch 1320/3000, Training Loss: 1.209e-03, Remain time: 9.8 min\n","Epoch 1330/3000, Training Loss: 8.155e-04, Remain time: 12.9 min\n","Epoch 1340/3000, Training Loss: 8.703e-04, Remain time: 9.2 min\n","Epoch 1350/3000, Training Loss: 8.427e-04, Remain time: 9.4 min\n","Epoch 1360/3000, Training Loss: 1.499e-03, Remain time: 9.2 min\n","Epoch 1370/3000, Training Loss: 1.071e-03, Remain time: 14.0 min\n","Epoch 1380/3000, Training Loss: 1.346e-03, Remain time: 9.0 min\n","Epoch 1390/3000, Training Loss: 8.928e-04, Remain time: 9.6 min\n","Epoch 1400/3000, Training Loss: 1.184e-03, Remain time: 8.8 min\n","Epoch 1410/3000, Training Loss: 1.296e-03, Remain time: 12.3 min\n","Epoch 1420/3000, Training Loss: 1.071e-03, Remain time: 9.0 min\n","Epoch 1430/3000, Training Loss: 1.264e-03, Remain time: 8.5 min\n","Epoch 1440/3000, Training Loss: 1.406e-03, Remain time: 8.5 min\n","Epoch 1450/3000, Training Loss: 1.080e-03, Remain time: 12.2 min\n","Epoch 1460/3000, Training Loss: 1.291e-03, Remain time: 8.4 min\n","Epoch 1470/3000, Training Loss: 1.148e-03, Remain time: 8.2 min\n","Epoch 1480/3000, Training Loss: 7.127e-04, Remain time: 8.4 min\n","Epoch 1490/3000, Training Loss: 1.075e-03, Remain time: 11.6 min\n","Epoch 1500/3000, Training Loss: 9.774e-04, Remain time: 8.2 min\n","Epoch 1510/3000, Training Loss: 1.144e-03, Remain time: 8.2 min\n","Epoch 1520/3000, Training Loss: 9.453e-04, Remain time: 8.4 min\n","Epoch 1530/3000, Training Loss: 1.493e-03, Remain time: 11.4 min\n","Epoch 1540/3000, Training Loss: 1.746e-03, Remain time: 7.9 min\n","Epoch 1550/3000, Training Loss: 9.479e-04, Remain time: 8.0 min\n","Epoch 1560/3000, Training Loss: 9.310e-04, Remain time: 7.9 min\n","Epoch 1570/3000, Training Loss: 1.773e-03, Remain time: 10.8 min\n","Epoch 1580/3000, Training Loss: 8.499e-04, Remain time: 7.4 min\n","Epoch 1590/3000, Training Loss: 8.729e-04, Remain time: 7.3 min\n","Epoch 1600/3000, Training Loss: 1.326e-03, Remain time: 7.3 min\n","Epoch 1610/3000, Training Loss: 9.453e-04, Remain time: 10.4 min\n","Epoch 1620/3000, Training Loss: 1.168e-03, Remain time: 7.4 min\n","Epoch 1630/3000, Training Loss: 1.270e-03, Remain time: 7.6 min\n","Epoch 1640/3000, Training Loss: 1.157e-03, Remain time: 7.9 min\n","Epoch 1650/3000, Training Loss: 9.315e-04, Remain time: 10.3 min\n","Epoch 1660/3000, Training Loss: 1.198e-03, Remain time: 7.0 min\n","Epoch 1670/3000, Training Loss: 8.091e-04, Remain time: 6.9 min\n","Epoch 1680/3000, Training Loss: 1.342e-03, Remain time: 7.1 min\n","Epoch 1690/3000, Training Loss: 8.673e-04, Remain time: 9.8 min\n","Epoch 1700/3000, Training Loss: 9.543e-04, Remain time: 8.5 min\n","Epoch 1710/3000, Training Loss: 9.087e-04, Remain time: 7.1 min\n","Epoch 1720/3000, Training Loss: 9.499e-04, Remain time: 6.5 min\n","Epoch 1730/3000, Training Loss: 1.025e-03, Remain time: 6.6 min\n","Epoch 1740/3000, Training Loss: 8.006e-04, Remain time: 9.0 min\n","Epoch 1750/3000, Training Loss: 1.493e-03, Remain time: 6.4 min\n","Epoch 1760/3000, Training Loss: 1.001e-03, Remain time: 6.4 min\n","Epoch 1770/3000, Training Loss: 1.932e-03, Remain time: 6.3 min\n","Epoch 1780/3000, Training Loss: 1.264e-03, Remain time: 9.0 min\n","Epoch 1790/3000, Training Loss: 8.378e-04, Remain time: 6.6 min\n","Epoch 1800/3000, Training Loss: 8.880e-04, Remain time: 6.2 min\n","Epoch 1810/3000, Training Loss: 6.216e-04, Remain time: 6.2 min\n","Epoch 1820/3000, Training Loss: 7.303e-04, Remain time: 8.9 min\n","Epoch 1830/3000, Training Loss: 7.461e-04, Remain time: 6.2 min\n","Epoch 1840/3000, Training Loss: 1.658e-03, Remain time: 6.0 min\n","Epoch 1850/3000, Training Loss: 9.344e-04, Remain time: 6.1 min\n","Epoch 1860/3000, Training Loss: 1.032e-03, Remain time: 8.8 min\n","Epoch 1870/3000, Training Loss: 8.554e-04, Remain time: 7.5 min\n","Epoch 1880/3000, Training Loss: 1.052e-03, Remain time: 5.6 min\n","Epoch 1890/3000, Training Loss: 9.725e-04, Remain time: 5.5 min\n","Epoch 1900/3000, Training Loss: 1.571e-03, Remain time: 5.7 min\n","Epoch 1910/3000, Training Loss: 1.242e-03, Remain time: 8.1 min\n","Epoch 1920/3000, Training Loss: 2.342e-03, Remain time: 5.4 min\n","Epoch 1930/3000, Training Loss: 1.357e-03, Remain time: 5.4 min\n","Epoch 1940/3000, Training Loss: 1.015e-03, Remain time: 5.4 min\n","Epoch 1950/3000, Training Loss: 2.093e-03, Remain time: 8.0 min\n","Epoch 1960/3000, Training Loss: 1.646e-03, Remain time: 5.3 min\n","Epoch 1970/3000, Training Loss: 9.988e-04, Remain time: 5.5 min\n","Epoch 1980/3000, Training Loss: 1.502e-03, Remain time: 5.4 min\n","Epoch 1990/3000, Training Loss: 8.643e-04, Remain time: 7.3 min\n","Epoch 2000/3000, Training Loss: 1.162e-03, Remain time: 5.1 min\n","Epoch 2010/3000, Training Loss: 9.790e-04, Remain time: 5.1 min\n","Epoch 2020/3000, Training Loss: 1.114e-03, Remain time: 5.1 min\n","Epoch 2030/3000, Training Loss: 1.370e-03, Remain time: 4.9 min\n","Epoch 2040/3000, Training Loss: 1.080e-03, Remain time: 6.8 min\n","Epoch 2050/3000, Training Loss: 1.208e-03, Remain time: 4.9 min\n","Epoch 2060/3000, Training Loss: 1.164e-03, Remain time: 4.7 min\n","Epoch 2070/3000, Training Loss: 1.074e-03, Remain time: 4.7 min\n","Epoch 2080/3000, Training Loss: 9.813e-04, Remain time: 6.6 min\n","Epoch 2090/3000, Training Loss: 1.382e-03, Remain time: 4.9 min\n","Epoch 2100/3000, Training Loss: 9.300e-04, Remain time: 4.7 min\n","Epoch 2110/3000, Training Loss: 1.059e-03, Remain time: 4.6 min\n","Epoch 2120/3000, Training Loss: 9.648e-04, Remain time: 6.4 min\n","Epoch 2130/3000, Training Loss: 1.162e-03, Remain time: 4.5 min\n","Epoch 2140/3000, Training Loss: 1.207e-03, Remain time: 4.2 min\n","Epoch 2150/3000, Training Loss: 8.458e-04, Remain time: 4.4 min\n","Epoch 2160/3000, Training Loss: 1.142e-03, Remain time: 5.7 min\n","Epoch 2170/3000, Training Loss: 1.517e-03, Remain time: 6.5 min\n","Epoch 2180/3000, Training Loss: 1.328e-03, Remain time: 4.1 min\n","Epoch 2190/3000, Training Loss: 1.208e-03, Remain time: 4.1 min\n","Epoch 2200/3000, Training Loss: 1.262e-03, Remain time: 4.2 min\n","Epoch 2210/3000, Training Loss: 1.094e-03, Remain time: 5.7 min\n","  Model saved , Validation Loss: 1.564e-03, lr: 7.445e-07\n","Epoch 2220/3000, Training Loss: 1.199e-03, Remain time: 4.1 min\n","Epoch 2230/3000, Training Loss: 1.184e-03, Remain time: 4.3 min\n","Epoch 2240/3000, Training Loss: 7.839e-04, Remain time: 3.8 min\n","Epoch 2250/3000, Training Loss: 1.235e-03, Remain time: 5.4 min\n","Epoch 2260/3000, Training Loss: 7.066e-04, Remain time: 3.7 min\n","Epoch 2270/3000, Training Loss: 1.037e-03, Remain time: 4.0 min\n","Epoch 2280/3000, Training Loss: 7.483e-04, Remain time: 3.7 min\n","Epoch 2290/3000, Training Loss: 1.113e-03, Remain time: 5.1 min\n","Epoch 2300/3000, Training Loss: 8.335e-04, Remain time: 3.5 min\n","Epoch 2310/3000, Training Loss: 1.490e-03, Remain time: 3.5 min\n","Epoch 2320/3000, Training Loss: 1.171e-03, Remain time: 3.5 min\n","Epoch 2330/3000, Training Loss: 1.537e-03, Remain time: 4.7 min\n","Epoch 2340/3000, Training Loss: 1.079e-03, Remain time: 4.1 min\n","Epoch 2350/3000, Training Loss: 1.080e-03, Remain time: 3.3 min\n","Epoch 2360/3000, Training Loss: 1.129e-03, Remain time: 4.6 min\n","Epoch 2370/3000, Training Loss: 1.195e-03, Remain time: 5.9 min\n","Epoch 2380/3000, Training Loss: 1.626e-03, Remain time: 3.3 min\n","Epoch 2390/3000, Training Loss: 1.743e-03, Remain time: 3.1 min\n","Epoch 2400/3000, Training Loss: 1.269e-03, Remain time: 3.2 min\n","Epoch 2410/3000, Training Loss: 1.252e-03, Remain time: 3.0 min\n","Epoch 2420/3000, Training Loss: 1.100e-03, Remain time: 4.2 min\n","Epoch 2430/3000, Training Loss: 8.053e-04, Remain time: 2.9 min\n","Epoch 2440/3000, Training Loss: 9.205e-04, Remain time: 2.9 min\n","Epoch 2450/3000, Training Loss: 1.085e-03, Remain time: 2.8 min\n","Epoch 2460/3000, Training Loss: 1.406e-03, Remain time: 3.8 min\n","Epoch 2470/3000, Training Loss: 1.424e-03, Remain time: 2.7 min\n","Epoch 2480/3000, Training Loss: 1.325e-03, Remain time: 2.6 min\n","Epoch 2490/3000, Training Loss: 9.741e-04, Remain time: 2.7 min\n","Epoch 2500/3000, Training Loss: 9.838e-04, Remain time: 3.7 min\n","Epoch 2510/3000, Training Loss: 1.772e-03, Remain time: 2.6 min\n","Epoch 2520/3000, Training Loss: 5.183e-04, Remain time: 2.4 min\n","Epoch 2530/3000, Training Loss: 1.662e-03, Remain time: 2.3 min\n","Epoch 2540/3000, Training Loss: 1.205e-03, Remain time: 3.4 min\n","Epoch 2550/3000, Training Loss: 8.870e-04, Remain time: 2.5 min\n","Epoch 2560/3000, Training Loss: 1.067e-03, Remain time: 2.2 min\n","Epoch 2570/3000, Training Loss: 1.210e-03, Remain time: 2.2 min\n","Epoch 2580/3000, Training Loss: 1.127e-03, Remain time: 2.1 min\n","Epoch 2590/3000, Training Loss: 9.051e-04, Remain time: 2.9 min\n","Epoch 2600/3000, Training Loss: 9.694e-04, Remain time: 2.1 min\n","Epoch 2610/3000, Training Loss: 1.039e-03, Remain time: 2.1 min\n","Epoch 2620/3000, Training Loss: 9.464e-04, Remain time: 2.1 min\n","Epoch 2630/3000, Training Loss: 1.175e-03, Remain time: 2.6 min\n","Epoch 2640/3000, Training Loss: 7.204e-04, Remain time: 1.9 min\n","Epoch 2650/3000, Training Loss: 1.563e-03, Remain time: 1.8 min\n","Epoch 2660/3000, Training Loss: 1.111e-03, Remain time: 1.8 min\n","Epoch 2670/3000, Training Loss: 1.472e-03, Remain time: 2.4 min\n","Epoch 2680/3000, Training Loss: 1.418e-03, Remain time: 1.6 min\n","Epoch 2690/3000, Training Loss: 6.175e-04, Remain time: 1.7 min\n","Epoch 2700/3000, Training Loss: 1.051e-03, Remain time: 1.6 min\n","Epoch 2710/3000, Training Loss: 1.131e-03, Remain time: 2.2 min\n","Epoch 2720/3000, Training Loss: 1.348e-03, Remain time: 1.8 min\n","Epoch 2730/3000, Training Loss: 1.557e-03, Remain time: 1.4 min\n","Epoch 2740/3000, Training Loss: 1.454e-03, Remain time: 1.3 min\n","Epoch 2750/3000, Training Loss: 1.579e-03, Remain time: 1.3 min\n","Epoch 2760/3000, Training Loss: 1.137e-03, Remain time: 1.7 min\n","Epoch 2770/3000, Training Loss: 1.220e-03, Remain time: 1.2 min\n","Epoch 2780/3000, Training Loss: 1.563e-03, Remain time: 1.1 min\n","Epoch 2790/3000, Training Loss: 8.689e-04, Remain time: 1.1 min\n","Epoch 2800/3000, Training Loss: 1.033e-03, Remain time: 1.4 min\n","Epoch 2810/3000, Training Loss: 1.136e-03, Remain time: 1.0 min\n","Epoch 2820/3000, Training Loss: 1.221e-03, Remain time: 0.9 min\n","Epoch 2830/3000, Training Loss: 8.092e-04, Remain time: 0.9 min\n","Epoch 2840/3000, Training Loss: 9.618e-04, Remain time: 1.1 min\n","Epoch 2850/3000, Training Loss: 8.924e-04, Remain time: 0.8 min\n","Epoch 2860/3000, Training Loss: 1.158e-03, Remain time: 0.7 min\n","Epoch 2870/3000, Training Loss: 1.258e-03, Remain time: 0.7 min\n","Epoch 2880/3000, Training Loss: 1.238e-03, Remain time: 0.9 min\n","Epoch 2890/3000, Training Loss: 9.066e-04, Remain time: 0.8 min\n","Epoch 2900/3000, Training Loss: 1.142e-03, Remain time: 0.5 min\n","Epoch 2910/3000, Training Loss: 1.019e-03, Remain time: 0.5 min\n","Epoch 2920/3000, Training Loss: 1.057e-03, Remain time: 0.4 min\n","Epoch 2930/3000, Training Loss: 1.606e-03, Remain time: 0.5 min\n","Epoch 2940/3000, Training Loss: 1.435e-03, Remain time: 0.3 min\n","Epoch 2950/3000, Training Loss: 1.283e-03, Remain time: 0.3 min\n","Epoch 2960/3000, Training Loss: 1.248e-03, Remain time: 0.2 min\n","Epoch 2970/3000, Training Loss: 1.094e-03, Remain time: 0.2 min\n","Epoch 2980/3000, Training Loss: 1.088e-03, Remain time: 0.1 min\n","Epoch 2990/3000, Training Loss: 8.535e-04, Remain time: 0.0 min\n","Epoch 3000/3000, Training Loss: 6.965e-04, Remain time: 0.0 min\n"]}],"source":["#train_model(epoch_num=800,lr=10e-4,method=\"forward\")\n","train_model(epoch_num=3000,lr=1e-4,method=\"forward\")"]},{"cell_type":"markdown","metadata":{"id":"Cszxdb72g9AO"},"source":["## GPU monitor\n","### nvidia-smi -l 3"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}