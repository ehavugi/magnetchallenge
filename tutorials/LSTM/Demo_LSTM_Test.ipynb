{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Network Testing \n",
        "\n",
        "This tutorial demonstrates how to test the LSTM-based model for the hysteresis loop prediction. The network model will be loaded through the saved state dictionary (.sd) file and the prediction results will be saved as (.csv) files.\n"
      ],
      "metadata": {
        "id": "wPItWytHXxuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Import Packages\n",
        "In this demo, the neural network is synthesized using the PyTorch framework. Please install PyTorch according to the [official guidance](https://pytorch.org/get-started/locally/) , then import PyTorch and other dependent modules."
      ],
      "metadata": {
        "id": "GQzQz2mkMqL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6IdDUEup6490"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Define Network Structure\n",
        "In this part, we define the structure of the LSTM-based encoder-projector-decoder neural network. Refer to the [PyTorch document](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) for more details."
      ],
      "metadata": {
        "id": "r9uAIku9M0Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model structures and functions\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "   def __init__(self, input_dim, hidden_dim):\n",
        "       super(Encoder, self).__init__()\n",
        "\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.input_dim = input_dim\n",
        "\n",
        "       self.lstm = nn.LSTM(1, self.hidden_dim, num_layers=1, batch_first=True)\n",
        "              \n",
        "   def forward(self, x):\n",
        "       outputs, (hidden, cell) = self.lstm(x)\n",
        "       return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "   def __init__(self, output_dim, hidden_dim):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.output_dim = output_dim\n",
        "\n",
        "       self.lstm = nn.LSTM(1, self.hidden_dim, num_layers=1, batch_first=True)\n",
        "       self.out = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim, self.hidden_dim*2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden_dim*2, self.output_dim))\n",
        "\n",
        "   def forward(self, x, hidden, cell):\n",
        "\n",
        "       batch = x.shape[0]\n",
        "       x = x.reshape(batch,1,1)\n",
        "       output, (hidden, cell) = self.lstm(x, (hidden, cell))     \n",
        "\n",
        "       prediction = self.out(output)\n",
        "       prediction = prediction.squeeze(0)\n",
        "      \n",
        "       return prediction, hidden, cell\n",
        "\n",
        "class Projector(nn.Module):\n",
        "   def __init__(self, num_var, hidden_dim, mod_dim):\n",
        "       super(Projector, self).__init__()\n",
        "\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.num_var = num_var\n",
        "       self.mod_dim = mod_dim\n",
        "\n",
        "       self.out = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim + self.num_var, self.mod_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.mod_dim, self.mod_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.mod_dim, self.hidden_dim))\n",
        "\n",
        "   def forward(self, x, var1):\n",
        "\n",
        "       x = x.squeeze(0)\n",
        "       y = self.out(torch.cat([x,var1],dim=1))\n",
        "       y = y.unsqueeze(0)\n",
        "      \n",
        "       return y\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, projector_hidden, projector_cell, decoder, device):\n",
        "       super().__init__()\n",
        "      \n",
        "       self.encoder = encoder\n",
        "       self.projector_hidden = projector_hidden\n",
        "       self.projector_cell = projector_cell\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target, var1, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       target_len = target.shape[1]\n",
        "       batch_size = source.shape[0]\n",
        "\n",
        "       trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "       outputs = torch.zeros(target_len+1, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "       hidden, cell = self.encoder(source)\n",
        "\n",
        "       hidden = self.projector_hidden(hidden, var1)\n",
        "       cell = self.projector_cell(hidden, var1)\n",
        "       \n",
        "       trg = torch.add(torch.zeros(batch_size, trg_vocab_size),10).to(self.device)\n",
        "\n",
        "       for t in range(1, target_len+1):   \n",
        "           prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
        "\n",
        "           outputs[t] = prediction.squeeze(2)\n",
        "\n",
        "           if random.random() < teacher_forcing_ratio:\n",
        "             trg = target[:,t-1]\n",
        "           else:\n",
        "             trg = prediction\n",
        "\n",
        "       return outputs[1:]\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "q5LA1kae860Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the Dataset\n",
        "In this part, we load and pre-process the dataset for the network training and testing. In this demo, a small dataset containing sinusoidal waveforms measured with N87 ferrite material under different frequency, temperature, and dc bias conditions is used, which can be downloaded from the [MagNet GitHub](https://github.com/PrincetonUniversity/Magnet) repository under \"tutorial\". "
      ],
      "metadata": {
        "id": "HxpYPTvPM6eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "def load_dataset(data_length=128):\n",
        "    # Load .json Files\n",
        "    with open('/content/Dataset_sine.json','r') as load_f:\n",
        "        DATA = json.load(load_f)\n",
        "    B = DATA['B_Field']\n",
        "    B = np.array(B)\n",
        "    Freq = DATA['Frequency']\n",
        "    Freq = np.log10(Freq)  # logarithm, optional\n",
        "    Temp = DATA['Temperature']\n",
        "    Temp = np.array(Temp)      \n",
        "    Hdc = DATA['Hdc']\n",
        "    Hdc = np.array(Hdc)       \n",
        "    H = DATA['H_Field']\n",
        "    H = np.array(H)\n",
        "\n",
        "    # Format data into tensors\n",
        "    in_B = torch.from_numpy(B).float().view(-1, data_length, 1)\n",
        "    in_F = torch.from_numpy(Freq).float().view(-1, 1)\n",
        "    in_T = torch.from_numpy(Temp).float().view(-1, 1)\n",
        "    in_D = torch.from_numpy(Hdc).float().view(-1, 1)\n",
        "    out_H = torch.from_numpy(H).float().view(-1, data_length, 1)\n",
        "\n",
        "    # Normalize\n",
        "    in_B = (in_B-torch.mean(in_B))/torch.std(in_B)\n",
        "    in_F = (in_F-torch.mean(in_F))/torch.std(in_F)\n",
        "    in_T = (in_T-torch.mean(in_T))/torch.std(in_T)\n",
        "    in_D = (in_D-torch.mean(in_D))/torch.std(in_D)\n",
        "    out_H = (out_H-torch.mean(out_H))/torch.std(out_H)\n",
        "\n",
        "    # Save the normalization coefficients for reproducing the output sequences\n",
        "    # For model deployment, all the coefficients need to be saved.\n",
        "    normH = [torch.mean(out_H),torch.std(out_H)]\n",
        "\n",
        "    print(in_B.size())\n",
        "    print(in_F.size())\n",
        "    print(in_T.size())\n",
        "    print(in_D.size())\n",
        "    print(out_H.size())\n",
        "\n",
        "    return torch.utils.data.TensorDataset(in_B, in_F, in_T, in_D, out_H), normH\n"
      ],
      "metadata": {
        "id": "Y-6sTpOoAUWZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Testing the Model\n",
        "In this part, we program the testing procedure of the network model. The loaded dataset is entirely used as the test set. The pre-trained model is loaded through the state dictionary file (.sd), and the output of the testing part is the (.csv) file containing the predicted sequences."
      ],
      "metadata": {
        "id": "1HRhRbOaM_ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config the model testing\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Reproducibility\n",
        "    random.seed(1)\n",
        "    np.random.seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    NUM_EPOCH = 2000\n",
        "    BATCH_SIZE = 128\n",
        "    DECAY_EPOCH = 150\n",
        "    DECAY_RATIO = 0.9\n",
        "    LR_INI = 0.004\n",
        "\n",
        "    # Select GPU as default device\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset, normH = load_dataset()\n",
        "    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "    # Setup network\n",
        "    encoder = Encoder(input_dim=100, hidden_dim=32).to(device)  \n",
        "    decoder = Decoder(output_dim=1, hidden_dim=32).to(device)  \n",
        "    projector_hidden = Projector(num_var=3, hidden_dim=32, mod_dim=64).to(device)  \n",
        "    projector_cell = Projector(num_var=3, hidden_dim=32, mod_dim=64).to(device)  \n",
        "    net = Seq2Seq(encoder, projector_hidden, projector_cell, decoder, device).to(device)  \n",
        "\n",
        "    # Load trained parameters\n",
        "    state_dict = torch.load('/content/Model_LSTM.sd')\n",
        "    net.load_state_dict(state_dict, strict=True)\n",
        "    net.eval()\n",
        "    print(\"Model is loaded!\")\n",
        "\n",
        "    # Log the number of parameters\n",
        "    print(\"Number of parameters: \", count_parameters(net))\n",
        "\n",
        "    # Test the network\n",
        "    with torch.no_grad():\n",
        "        for in_B, in_F, in_T, in_D, out_H in test_loader:\n",
        "\n",
        "            outputs = net(in_B.to(device),out_H.to(device),torch.cat((in_F.to(device), in_T.to(device), in_D.to(device)), dim=1),teacher_forcing_ratio=0.0)\n",
        "            outputs = outputs.transpose(0,1)\n",
        "\n",
        "            # Save results\n",
        "            with open(\"/content/pred.csv\", \"a\") as f:\n",
        "                np.savetxt(f, (outputs*normH[1]+normH[0]).squeeze(2).cpu().numpy())\n",
        "                f.close()\n",
        "            with open(\"/content/meas.csv\", \"a\") as f:\n",
        "                np.savetxt(f, (out_H*normH[1]+normH[0]).squeeze(2).cpu().numpy())\n",
        "                f.close()\n",
        "\n",
        "        print(\"Testing finished! Results are saved!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ndXYTq9b9R",
        "outputId": "ca4c5143-1564-4f20-aa29-68fc985426c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3495, 128, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 128, 1])\n",
            "Model is loaded!\n",
            "Number of parameters:  28225\n",
            "Testing finished! Results are saved!\n"
          ]
        }
      ]
    }
  ]
}