{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Network Training \n",
        "This tutorial demonstrates how to train the transformer-based model for the hysteresis loop prediction. The network model will be trained based on Dataset_sine.json and saved as a state dictionary (.sd) file.\n"
      ],
      "metadata": {
        "id": "JRKtl4ciW12L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Import Packages\n",
        "In this demo, the neural network is synthesized using the PyTorch framework. Please install PyTorch according to the [official guidance](https://pytorch.org/get-started/locally/) , then import PyTorch and other dependent modules."
      ],
      "metadata": {
        "id": "GQzQz2mkMqL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IdDUEup6490"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Define Network Structure\n",
        "In this part, we define the structure of the transformer-based encoder-projector-decoder neural network. Refer to the [PyTorch document](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) for more details."
      ],
      "metadata": {
        "id": "r9uAIku9M0Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model structures and functions\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, \n",
        "        input_size :int,\n",
        "        dec_seq_len :int,\n",
        "        max_seq_len :int,\n",
        "        out_seq_len :int,\n",
        "        dim_val :int,  \n",
        "        n_encoder_layers :int,\n",
        "        n_decoder_layers :int,\n",
        "        n_heads :int,\n",
        "        dropout_encoder,\n",
        "        dropout_decoder,\n",
        "        dropout_pos_enc,\n",
        "        dim_feedforward_encoder :int,\n",
        "        dim_feedforward_decoder :int,\n",
        "        dim_feedforward_projecter :int,\n",
        "        num_var: int=3\n",
        "        ): \n",
        "\n",
        "        #   Args:\n",
        "        #    input_size: int, number of input variables. 1 if univariate.\n",
        "        #    dec_seq_len: int, the length of the input sequence fed to the decoder\n",
        "        #    max_seq_len: int, length of the longest sequence the model will receive. Used in positional encoding. \n",
        "        #    out_seq_len: int, the length of the model's output (i.e. the target sequence length)\n",
        "        #    dim_val: int, aka d_model. All sub-layers in the model produce outputs of dimension dim_val\n",
        "        #    n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
        "        #    n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
        "        #    n_heads: int, the number of attention heads (aka parallel attention layers)\n",
        "        #    dropout_encoder: float, the dropout rate of the encoder\n",
        "        #    dropout_decoder: float, the dropout rate of the decoder\n",
        "        #    dropout_pos_enc: float, the dropout rate of the positional encoder\n",
        "        #    dim_feedforward_encoder: int, number of neurons in the linear layer of the encoder\n",
        "        #    dim_feedforward_decoder: int, number of neurons in the linear layer of the decoder\n",
        "        #    dim_feedforward_projecter :int, number of neurons in the linear layer of the projecter\n",
        "        #    num_var: int, number of additional input variables of the projector\n",
        "\n",
        "        super().__init__() \n",
        "\n",
        "        self.dec_seq_len = dec_seq_len\n",
        "        self.n_heads = n_heads\n",
        "        self.out_seq_len = out_seq_len\n",
        "        self.dim_val = dim_val\n",
        "        self.encoder_input_layer = nn.Sequential(\n",
        "            nn.Linear(input_size, dim_val),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim_val, dim_val))\n",
        "        self.decoder_input_layer = nn.Sequential(\n",
        "            nn.Linear(input_size, dim_val),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim_val, dim_val))\n",
        "        self.linear_mapping = nn.Sequential(\n",
        "            nn.Linear(dim_val, dim_val),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim_val, input_size))\n",
        "        self.positional_encoding_layer = PositionalEncoder(d_model=dim_val, dropout=dropout_pos_enc, max_len=max_seq_len)\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(dim_val + num_var, dim_feedforward_projecter),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim_feedforward_projecter, dim_feedforward_projecter),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(dim_feedforward_projecter, dim_val))\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=dim_val, \n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward_encoder,\n",
        "            dropout=dropout_encoder,\n",
        "            activation=\"relu\",\n",
        "            batch_first=True\n",
        "            )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layer, num_layers=n_encoder_layers, norm=None)\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=dim_val,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=dim_feedforward_decoder,\n",
        "            dropout=dropout_decoder,\n",
        "            activation=\"relu\",\n",
        "            batch_first=True\n",
        "            )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer=self.decoder_layer, num_layers=n_decoder_layers, norm=None)\n",
        "\n",
        "    def forward(self, src: Tensor, tgt: Tensor, var: Tensor, device) -> Tensor:\n",
        "\n",
        "        src = self.encoder_input_layer(src)\n",
        "        src = self.positional_encoding_layer(src)\n",
        "        src = self.encoder(src)\n",
        "        enc_seq_len = 128\n",
        "\n",
        "        var = var.unsqueeze(1).repeat(1,enc_seq_len,1)\n",
        "        src = self.projector(torch.cat([src,var],dim=2))\n",
        "\n",
        "        tgt = self.decoder_input_layer(tgt)\n",
        "        tgt = self.positional_encoding_layer(tgt)\n",
        "        batch_size = src.size()[0]\n",
        "        tgt_mask = generate_square_subsequent_mask(sz1=self.out_seq_len, sz2=self.out_seq_len).to(device)\n",
        "        output = self.decoder(\n",
        "            tgt=tgt,\n",
        "            memory=src,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=None\n",
        "            ) \n",
        "        output= self.linear_mapping(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x + self.pe[:x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "def generate_square_subsequent_mask(sz1: int, sz2: int) -> Tensor:\n",
        "    #Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
        "    return torch.triu(torch.ones(sz1, sz2) * float('-inf'), diagonal=1)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "q5LA1kae860Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the Dataset\n",
        "In this part, we load and pre-process the dataset for the network training and testing. In this demo, a small dataset containing sinusoidal waveforms measured with N87 ferrite material under different frequency, temperature, and dc bias conditions is used, which can be downloaded from the [MagNet GitHub](https://github.com/PrincetonUniversity/Magnet) repository under \"tutorial\". "
      ],
      "metadata": {
        "id": "HxpYPTvPM6eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "def load_dataset(data_length=128):\n",
        "    # Load .json Files\n",
        "    with open('/content/Dataset_sine.json','r') as load_f:\n",
        "        DATA = json.load(load_f)\n",
        "    B = DATA['B_Field']\n",
        "    B = np.array(B)\n",
        "    Freq = DATA['Frequency']\n",
        "    Freq = np.log10(Freq)  # logarithm, optional\n",
        "    Temp = DATA['Temperature']\n",
        "    Temp = np.array(Temp)      \n",
        "    Hdc = DATA['Hdc']\n",
        "    Hdc = np.array(Hdc)       \n",
        "    H = DATA['H_Field']\n",
        "    H = np.array(H)\n",
        "\n",
        "    # Format data into tensors\n",
        "    in_B = torch.from_numpy(B).float().view(-1, data_length, 1)\n",
        "    in_F = torch.from_numpy(Freq).float().view(-1, 1)\n",
        "    in_T = torch.from_numpy(Temp).float().view(-1, 1)\n",
        "    in_D = torch.from_numpy(Hdc).float().view(-1, 1)\n",
        "    out_H = torch.from_numpy(H).float().view(-1, data_length, 1)\n",
        "\n",
        "    # Normalize\n",
        "    in_B = (in_B-torch.mean(in_B))/torch.std(in_B)\n",
        "    in_F = (in_F-torch.mean(in_F))/torch.std(in_F)\n",
        "    in_T = (in_T-torch.mean(in_T))/torch.std(in_T)\n",
        "    in_D = (in_D-torch.mean(in_D))/torch.std(in_D)\n",
        "    out_H = (out_H-torch.mean(out_H))/torch.std(out_H)\n",
        "\n",
        "    # Save the normalization coefficients for reproducing the output sequences\n",
        "    # For model deployment, all the coefficients need to be saved.\n",
        "    normH = [torch.mean(out_H),torch.std(out_H)]\n",
        "\n",
        "    # Attach the starting token and add the noise\n",
        "    head = 0.1 * torch.ones(out_H.size()[0],1,out_H.size()[2])\n",
        "    out_H_head = torch.cat((head,out_H), dim=1)\n",
        "    out_H = out_H_head\n",
        "    out_H_head = out_H_head + (torch.rand(out_H_head.size())-0.5)*0.1\n",
        "\n",
        "    print(in_B.size())\n",
        "    print(in_F.size())\n",
        "    print(in_T.size())\n",
        "    print(in_D.size())\n",
        "    print(out_H.size())\n",
        "    print(out_H_head.size())\n",
        "\n",
        "    return torch.utils.data.TensorDataset(in_B, in_F, in_T, in_D, out_H, out_H_head), normH\n"
      ],
      "metadata": {
        "id": "Y-6sTpOoAUWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Training the Model\n",
        "In this part, we program the training procedure of the network model. The loaded dataset is randomly split into training set and validation set. The output of the training is the state dictionary file (.sd) containing all the trained parameter values."
      ],
      "metadata": {
        "id": "1HRhRbOaM_ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config the model training\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Reproducibility\n",
        "    random.seed(1)\n",
        "    np.random.seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    NUM_EPOCH = 2000\n",
        "    BATCH_SIZE = 128\n",
        "    DECAY_EPOCH = 150\n",
        "    DECAY_RATIO = 0.9\n",
        "    LR_INI = 0.004\n",
        "\n",
        "    # Select GPU as default device\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset, normH = load_dataset()\n",
        "\n",
        "    # Split the dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "    # Setup network\n",
        "    net = Transformer(\n",
        "      dim_val=24,\n",
        "      input_size=1, \n",
        "      dec_seq_len=129,\n",
        "      max_seq_len=129,\n",
        "      out_seq_len=129, \n",
        "      n_decoder_layers=1,\n",
        "      n_encoder_layers=1,\n",
        "      n_heads=4,\n",
        "      dropout_encoder=0.0, \n",
        "      dropout_decoder=0.0,\n",
        "      dropout_pos_enc=0.0,\n",
        "      dim_feedforward_encoder=40,\n",
        "      dim_feedforward_decoder=40,\n",
        "      dim_feedforward_projecter=40).to(device)\n",
        "\n",
        "    # Log the number of parameters\n",
        "    print(\"Number of parameters: \", count_parameters(net))\n",
        "\n",
        "    # Setup optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LR_INI) \n",
        "\n",
        "    # Train the network\n",
        "    for epoch_i in range(NUM_EPOCH):\n",
        "\n",
        "        # Train for one epoch\n",
        "        epoch_train_loss = 0\n",
        "        net.train()\n",
        "        optimizer.param_groups[0]['lr'] = LR_INI* (DECAY_RATIO ** (0+ epoch_i // DECAY_EPOCH))\n",
        "\n",
        "        for in_B, in_F, in_T, in_D, out_H, out_H_head in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = net(src=in_B.to(device), tgt=out_H_head.to(device), var=torch.cat((in_F.to(device), in_T.to(device), in_D.to(device)), dim=1), device=device)\n",
        "            loss = criterion(output[:,:-1,:], out_H.to(device)[:,1:,:])\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.25)\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Compute validation\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            epoch_valid_loss = 0\n",
        "            for in_B, in_F, in_T, in_D, out_H, out_H_head in valid_loader:\n",
        "                output = net(src=in_B.to(device), tgt=out_H_head.to(device), var=torch.cat((in_F.to(device), in_T.to(device), in_D.to(device)), dim=1), device=device)\n",
        "                loss = criterion(output[:,:-1,:], out_H.to(device)[:,1:,:])\n",
        "                epoch_valid_loss += loss.item()\n",
        "        \n",
        "        if (epoch_i+1)%200 == 0:\n",
        "          print(f\"Epoch {epoch_i+1:2d} \"\n",
        "              f\"Train {epoch_train_loss / len(train_dataset) * 1e5:.5f} \"\n",
        "              f\"Valid {epoch_valid_loss / len(valid_dataset) * 1e5:.5f}\")\n",
        "        \n",
        "    # Save the model parameters\n",
        "    torch.save(net.state_dict(), \"/content/Model_Transformer.sd\")\n",
        "    print(\"Training finished! Model is saved!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ndXYTq9b9R",
        "outputId": "2b870a56-52d4-47ce-f52f-39687cbad098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3495, 128, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 129, 1])\n",
            "torch.Size([3495, 129, 1])\n",
            "Number of parameters:  28481\n",
            "Epoch 200 Train 0.26933 Valid 0.26641\n",
            "Epoch 400 Train 0.32441 Valid 0.39098\n",
            "Epoch 600 Train 0.13355 Valid 0.21689\n",
            "Epoch 800 Train 0.15760 Valid 0.18231\n",
            "Epoch 1000 Train 0.09614 Valid 0.14578\n",
            "Epoch 1200 Train 0.13341 Valid 0.14217\n",
            "Epoch 1400 Train 0.07395 Valid 0.09841\n",
            "Epoch 1600 Train 0.07107 Valid 0.12229\n",
            "Epoch 1800 Train 0.06711 Valid 0.07995\n",
            "Epoch 2000 Train 0.05829 Valid 0.07671\n",
            "Training finished! Model is saved!\n"
          ]
        }
      ]
    }
  ]
}