{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Network Training \n",
        "This tutorial demonstrates how to train the LSTM-based model for the hysteresis loop prediction. The network model will be trained based on Dataset_sine.json and saved as a state dictionary (.sd) file.\n"
      ],
      "metadata": {
        "id": "JRKtl4ciW12L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Import Packages\n",
        "In this demo, the neural network is synthesized using the PyTorch framework. Please install PyTorch according to the [official guidance](https://pytorch.org/get-started/locally/) , then import PyTorch and other dependent modules."
      ],
      "metadata": {
        "id": "GQzQz2mkMqL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IdDUEup6490"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Define Network Structure\n",
        "In this part, we define the structure of the LSTM-based encoder-projector-decoder neural network. Refer to the [PyTorch document](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) for more details."
      ],
      "metadata": {
        "id": "r9uAIku9M0Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model structures and functions\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "   def __init__(self, input_dim, hidden_dim):\n",
        "       super(Encoder, self).__init__()\n",
        "\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.input_dim = input_dim\n",
        "\n",
        "       self.lstm = nn.LSTM(1, self.hidden_dim, num_layers=1, batch_first=True)\n",
        "              \n",
        "   def forward(self, x):\n",
        "       outputs, (hidden, cell) = self.lstm(x)\n",
        "       return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "   def __init__(self, output_dim, hidden_dim):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.output_dim = output_dim\n",
        "\n",
        "       self.lstm = nn.LSTM(1, self.hidden_dim, num_layers=1, batch_first=True)\n",
        "       self.out = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim, self.hidden_dim*2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden_dim*2, self.output_dim))\n",
        "\n",
        "   def forward(self, x, hidden, cell):\n",
        "\n",
        "       batch = x.shape[0]\n",
        "       x = x.reshape(batch,1,1)\n",
        "       output, (hidden, cell) = self.lstm(x, (hidden, cell))     \n",
        "\n",
        "       prediction = self.out(output)\n",
        "       prediction = prediction.squeeze(0)\n",
        "      \n",
        "       return prediction, hidden, cell\n",
        "\n",
        "class Projector(nn.Module):\n",
        "   def __init__(self, num_var, hidden_dim, mod_dim):\n",
        "       super(Projector, self).__init__()\n",
        "\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.num_var = num_var\n",
        "       self.mod_dim = mod_dim\n",
        "\n",
        "       self.out = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dim + self.num_var, self.mod_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.mod_dim, self.mod_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.mod_dim, self.hidden_dim))\n",
        "\n",
        "   def forward(self, x, var1):\n",
        "\n",
        "       x = x.squeeze(0)\n",
        "       y = self.out(torch.cat([x,var1],dim=1))\n",
        "       y = y.unsqueeze(0)\n",
        "      \n",
        "       return y\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, projector_hidden, projector_cell, decoder, device):\n",
        "       super().__init__()\n",
        "      \n",
        "       self.encoder = encoder\n",
        "       self.projector_hidden = projector_hidden\n",
        "       self.projector_cell = projector_cell\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target, var1, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       target_len = target.shape[1]\n",
        "       batch_size = source.shape[0]\n",
        "\n",
        "       trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "       outputs = torch.zeros(target_len+1, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "       hidden, cell = self.encoder(source)\n",
        "\n",
        "       hidden = self.projector_hidden(hidden, var1)\n",
        "       cell = self.projector_cell(hidden, var1)\n",
        "       \n",
        "       trg = torch.add(torch.zeros(batch_size, trg_vocab_size),10).to(self.device)\n",
        "\n",
        "       for t in range(1, target_len+1):   \n",
        "           prediction, hidden, cell = self.decoder(trg, hidden, cell)\n",
        "\n",
        "           outputs[t] = prediction.squeeze(2)\n",
        "\n",
        "           if random.random() < teacher_forcing_ratio:\n",
        "             trg = target[:,t-1]\n",
        "           else:\n",
        "             trg = prediction\n",
        "\n",
        "       return outputs[1:]\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ],
      "metadata": {
        "id": "q5LA1kae860Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Load the Dataset\n",
        "In this part, we load and pre-process the dataset for the network training and testing. In this demo, a small dataset containing sinusoidal waveforms measured with N87 ferrite material under different frequency, temperature, and dc bias conditions is used, which can be downloaded from the [MagNet GitHub](https://github.com/PrincetonUniversity/Magnet) repository under \"tutorial\". "
      ],
      "metadata": {
        "id": "HxpYPTvPM6eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "\n",
        "def load_dataset(data_length=128):\n",
        "    # Load .json Files\n",
        "    with open('/content/Dataset_sine.json','r') as load_f:\n",
        "        DATA = json.load(load_f)\n",
        "    B = DATA['B_Field']\n",
        "    B = np.array(B)\n",
        "    Freq = DATA['Frequency']\n",
        "    Freq = np.log10(Freq)  # logarithm, optional\n",
        "    Temp = DATA['Temperature']\n",
        "    Temp = np.array(Temp)      \n",
        "    Hdc = DATA['Hdc']\n",
        "    Hdc = np.array(Hdc)       \n",
        "    H = DATA['H_Field']\n",
        "    H = np.array(H)\n",
        "\n",
        "    # Format data into tensors\n",
        "    in_B = torch.from_numpy(B).float().view(-1, data_length, 1)\n",
        "    in_F = torch.from_numpy(Freq).float().view(-1, 1)\n",
        "    in_T = torch.from_numpy(Temp).float().view(-1, 1)\n",
        "    in_D = torch.from_numpy(Hdc).float().view(-1, 1)\n",
        "    out_H = torch.from_numpy(H).float().view(-1, data_length, 1)\n",
        "\n",
        "    # Normalize\n",
        "    in_B = (in_B-torch.mean(in_B))/torch.std(in_B)\n",
        "    in_F = (in_F-torch.mean(in_F))/torch.std(in_F)\n",
        "    in_T = (in_T-torch.mean(in_T))/torch.std(in_T)\n",
        "    in_D = (in_D-torch.mean(in_D))/torch.std(in_D)\n",
        "    out_H = (out_H-torch.mean(out_H))/torch.std(out_H)\n",
        "\n",
        "    # Save the normalization coefficients for reproducing the output sequences\n",
        "    # For model deployment, all the coefficients need to be saved.\n",
        "    normH = [torch.mean(out_H),torch.std(out_H)]\n",
        "\n",
        "    print(in_B.size())\n",
        "    print(in_F.size())\n",
        "    print(in_T.size())\n",
        "    print(in_D.size())\n",
        "    print(out_H.size())\n",
        "\n",
        "    return torch.utils.data.TensorDataset(in_B, in_F, in_T, in_D, out_H), normH\n"
      ],
      "metadata": {
        "id": "Y-6sTpOoAUWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Training the Model\n",
        "In this part, we program the training procedure of the network model. The loaded dataset is randomly split into training set and validation set. The output of the training is the state dictionary file (.sd) containing all the trained parameter values."
      ],
      "metadata": {
        "id": "1HRhRbOaM_ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config the model training\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Reproducibility\n",
        "    random.seed(1)\n",
        "    np.random.seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Hyperparameters\n",
        "    NUM_EPOCH = 2000\n",
        "    BATCH_SIZE = 128\n",
        "    DECAY_EPOCH = 150\n",
        "    DECAY_RATIO = 0.9\n",
        "    LR_INI = 0.004\n",
        "\n",
        "    # Select GPU as default device\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset, normH = load_dataset()\n",
        "\n",
        "    # Split the dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
        "\n",
        "    # Setup network\n",
        "    encoder = Encoder(input_dim=100, hidden_dim=32).to(device)  \n",
        "    decoder = Decoder(output_dim=1, hidden_dim=32).to(device)  \n",
        "    projector_hidden = Projector(num_var=3, hidden_dim=32, mod_dim=64).to(device)  \n",
        "    projector_cell = Projector(num_var=3, hidden_dim=32, mod_dim=64).to(device)  \n",
        "    net = Seq2Seq(encoder, projector_hidden, projector_cell, decoder, device).to(device)  \n",
        "\n",
        "    # Log the number of parameters\n",
        "    print(\"Number of parameters: \", count_parameters(net))\n",
        "\n",
        "    # Setup optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LR_INI) \n",
        "\n",
        "    # Train the network\n",
        "    for epoch_i in range(NUM_EPOCH):\n",
        "\n",
        "        # Train for one epoch\n",
        "        epoch_train_loss = 0\n",
        "        net.train()\n",
        "        optimizer.param_groups[0]['lr'] = LR_INI* (DECAY_RATIO ** (0+ epoch_i // DECAY_EPOCH))\n",
        "\n",
        "        for in_B, in_F, in_T, in_D, out_H in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(in_B.to(device),out_H.to(device),torch.cat((in_F.to(device), in_T.to(device), in_D.to(device)), dim=1),teacher_forcing_ratio=0.0)\n",
        "            loss = criterion(outputs, out_H.transpose(0,1).to(device))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        # Compute validation\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            epoch_valid_loss = 0\n",
        "            for in_B, in_F, in_T, in_D, out_H in valid_loader:\n",
        "                outputs = net(in_B.to(device),out_H.to(device),torch.cat((in_F.to(device), in_T.to(device), in_D.to(device)), dim=1),teacher_forcing_ratio=0)\n",
        "                loss = criterion(outputs, out_H.transpose(0,1).to(device))\n",
        "                epoch_valid_loss += loss.item()\n",
        "        \n",
        "        if (epoch_i+1)%200 == 0:\n",
        "          print(f\"Epoch {epoch_i+1:2d} \"\n",
        "              f\"Train {epoch_train_loss / len(train_dataset) * 1e5:.5f} \"\n",
        "              f\"Valid {epoch_valid_loss / len(valid_dataset) * 1e5:.5f}\")\n",
        "        \n",
        "    # Save the model parameters\n",
        "    torch.save(net.state_dict(), \"/content/Model_LSTM.sd\")\n",
        "    print(\"Training finished! Model is saved!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ndXYTq9b9R",
        "outputId": "f74dd0c3-4097-4487-9a6e-602154a7f838"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3495, 128, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 1])\n",
            "torch.Size([3495, 128, 1])\n",
            "Number of parameters:  28225\n",
            "Epoch 200 Train 5.16456 Valid 4.64060\n",
            "Epoch 400 Train 1.34637 Valid 1.70746\n",
            "Epoch 600 Train 1.02441 Valid 1.69500\n",
            "Epoch 800 Train 1.22304 Valid 2.10886\n",
            "Epoch 1000 Train 0.49161 Valid 1.23090\n",
            "Epoch 1200 Train 0.27401 Valid 0.96702\n",
            "Epoch 1400 Train 0.28243 Valid 1.11031\n",
            "Epoch 1600 Train 0.25291 Valid 0.91943\n",
            "Epoch 1800 Train 0.18748 Valid 0.92806\n",
            "Epoch 2000 Train 0.17573 Valid 0.77812\n",
            "Training finished! Model is saved!\n"
          ]
        }
      ]
    }
  ]
}